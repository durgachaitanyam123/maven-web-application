About Kernel:
The Linux® kernel is the main component of a Linux operating system (OS) and is the core interface between a computer’s hardware and its processes. It communicates between the 2, managing resources as efficiently as possible.

The kernel is so named because—like a seed inside a hard shell—it exists within the OS and controls all the major functions of the hardware, whether it’s a phone, laptop, server, or any other kind of computer.

What the kernel does
The kernel has 4 jobs:

Memory management: Keep track of how much memory is used to store what, and where

Process management: Determine which processes can use the central processing unit (CPU), when, and for how long

Device drivers: Act as mediator/interpreter between the hardware and processes

System calls and security: Receive requests for service from the processes
The kernel, if implemented properly, is invisible to the user, working in its own little world known as kernel space, where it allocates memory and keeps track of where everything is stored. What the user sees—like web browsers and files—are known as the user space. These applications interact with the kernel through a system call interface (SCI).

Linux Directories:
/etc: The /etc directory contains system configuration files for all the services, scripts, and third-party applications that are installed. This directory is considered the nerve center of the Linux system.

/home: The /home directory is a directory that contains a user’s personal folders and files. On a graphical Linux system, the home directory, by default, contains folders such as Desktop, Documents, Downloads, Pictures, Videos, and Public.

In addition, the /home directory contains personal configuration files which are prefixed with a dot (.). These are hidden files that contain user-specific settings for the login shell session.

/var: The /var directory stores system-generated variable files, which include log files, caches, and spool files just to mention a few.

/usr: The /usr directory ranks as one of the most important directories due to the enormous amount of data it holds. The directory contains system-wide read-only files. These include libraries, user binaries and their documentation, programs, and system utilities.

/tmp: The /tmp directory is a directory that stores temporary files and many programs use this directory to create lock files and keep the temporary storage of data. Do not delete files under the/tmp directory unless you know exactly what you are doing! Many of these files are critical for presently running programs and removing them may affect a system crash.

/bin: The /bin directory contains user binaries, executable programs, and common system commands that are used by all users in the system. These include ls, pwd, cat, mkdir, cd, mv, cp, du, df, tar, rpm, wc, history, etc.

/opt: The /opt directory is a directory that is reserved for add-on packages and third-party software applications that are not included by default in the system’s official repositories.
For example, when you install applications such as Skype, Discord, Spotify, and Java, to mention a few, they get stored in the /opt directory.



Linux Commands:     {Command FlagName FileName}
1. whoami
2. pwd
3. cd ..
4. cat
5. clear
5. sudo -i
6. su username {Switch User}
7. useradd username
8. passwd username
9. userdel username
10. groupadd groupname
11. usermod -a -G groupname username
12. groupdel groupname
13. mkdir Dir_Name
14. rmdir Dir_Name
15. touch filename
16. rm filename
17. ls
18. ls -l or ll {Longlist}
18. ls -a
19. vi editor(you can write something in File)
20. id username/groupname
21. cp filename /pathname
22. mv filename /pathname {To change file name or moving the file path}
23. hostnamectl set-hostname hostname
24. cat /etc/os-release
25. cat /etc/passwd
26. cat /etc/group
27. man command_name {To Know Brief About The particular command}
28. vi /etc/sudoers {How to give Sudoers file permissions to users}
29. chmod permissions filename
30. chown username filename
31. stat filename : this file prints the more info about the file
32. lsof -u username
33. free -m {Memory}
34. df -h {Filesystem}
35. history
36. [root@devops /]# getent group AB
37. vi /etc/ssh/sshd_config {To Make Configurations}
38. systemctl restart sshd { For Restarting File to updtae the changes}
39. rpm -qa {To cehck the number of applications installed in your system}
40. cmp  {Allows you to check if two files are identical}
41. usermod {change existing users data}
42. whatis {Find what a command is used for}
43. diff {Find the difference between two files}
44. alias {Create custom shortcuts for your regularly used commands}
45.  less  {Linux command to display paged outputs in the terminal}
46. cat > filename {To add some data into the file}
47. ifconfig
48. netstat
49. top (It prints current ruuning processes in server and also we can cpu utilization and ram utilization)

AB:x:1004:A1,A2,A3,A4
Topic vi editor Insert Mode:
i - Start Writing from one step from the current position
a - Start Writing from the current position
I - Start Writing from beginning 
o - insert new line below the current line
O - insert new line above the current line

ESC Mode
G - To go to last line
gg - go to first line
 5gg - go to 5th line
yy - To copy the currenr line
  5yy - copy 5lines from the current line
p - Paste the copied line below the current line
   5p - paste the copied content  times below the current line
dd - delete the current line
   5dd - delete the 5 lines from current line
ctrl + u - undo the previous action
ctrl + r - redo the previous action
:se nu
:q! - Quit
:w - To save the changes
:wq! - To save and quit from vi editor
  


Task: 1.Command to print the output of users in particular group?
      2.Users in particular Group should have sudoers permission? {Dont give sudoers permission to users}
      3. Adding user and giving paaswd to him. Now, is it possible for him to access the server from another system.
2things to remember: 1.To make configuration
                     2. To restart the confiuration file
      
       4. How to zip and un-zip tar files in linux
          tar -cvf filename foldername
          tar -xvf tar-file
       5. Is it possible to concatenate two commands or Is it possible to give 2 commands at a time?

          How to limit the sudoers permission to users

Today's Discussion:

ps
kill
YUM(YellowDog Updater MOdified)  
curl and wget
rpm -qa (If i want to see particular app use command rpm -qa | grep appname
head command
tail command
sed command is used for Modifying the data in the file { sed 's/olddata/newdata' filename }
grep command {grep -c/-n/-w "string" filename}
sort command {sort -r/-n filename}
find command {find / -type d -name abc} {find /home -iname homw.txt} {find / -perm /u=r}
wc command(word count) it prints number of lines, words and character in a file {wc filename} {-c filename} -c: bytes

##################################################################################################

TASK: Print Filename Which are created since 2Days
      Print Filename Which are more than 10mb and less than 15mb
      print Files with amar group?
      How to print the other lines excluding first 2 lines {use head command} ps -ef | head -n -2
      How many duplicates arein file?
      Which line has the duplicate data?
       
      
      


Today's Topic:

Text Processing Tools:

0. grep: grep -i -w stringname filename {it removes the duplicates}
         grep -i ^stringname filename {it prints lefthand first words}
         ps -ef | grep filename(java)

1. find: find  / -name os-release
         find  / -name passwd
         find  / -name *.txt
         find  / -size +10M
         find  / -atime +2
         find  / -ctime +2 (2daysbefore)
2. echo: It will Print Messages which you given
3. tr:   The tr command is a UNIX command-line utility for translating or deleting characters.
         cat greekfile | tr [a-z] [A-Z]
         echo "{NEW one}" | tr "{}" "()"
         echo "Welcome    To    INDIA" | tr -s " "
         echo "Sorry to say this, You are not eligible" | tr -d "not"
4. sort: SORT command is used to sort a file, arranging the records in a particular order.
         sort filename
         sort -r filename {reverese format}
         sort -n filename {for numerical data}
         sort -nr filename
         sort f2>f3
         sort -o f2 f3
         cat f3
5. uniq: The uniq command in Linux is a command-line utility that reports or filters out the repeated lines in a file.
         uniq -c f2
         uniq -u f2 (it prints only unique lines)
         uniq -d f2 (it prints only repeated lines)
6. awk:  extract the fields/columns based on some deliminator/separator from the output
          free -m | grep Mem | awk -F " " '{print $7}'
          uname -a|awk -F " " '{print $3}'|awk -F '.' '{print $5}'
         
7. uname -a: To know the kernel version
8. shell: it is like interpreter. Means it takes input and send it to the kernel. Green color cursor is the representer.
               COMMAND:     cat /etc/shells
9. cut: cut command is also like awk command but small differences are there.
 df -m /|tail -n1|cut -d ' ' -f1 

################################################################################
Today's Topic Shell Script:

*. shell: it is like interpreter. Means it takes input and send it to the kernel. Green color cursor is the representer.
               COMMAND:     cat /etc/shells

0. cut: cut command is also like awk command but small differences are there.
 df -m /|tail -n1|cut -d ' ' -f1 

1. create simple_script.sh

2. vi simple_script.sh

#!/bin/bash
echo "This is My First Script Team"
:wq!

3. cat simple_script.sh

4. ls -l simple_script.sh

5. chmod +x simple_script.sh

6. ./simple_script.sh or /simple_script.sh or sh simple_script.sh

7. Suppose you are in some other location and you want to run shell script.
   there you need to give path of s
RUN TIME VARIABLE:

#!/bin/bash
echo "Please provide value for a"
read a
echo "Please provide value for b"
read b
expr $a + $b

POSITIONAL PARAMETER

#!/bin/bash
a=$1
b=$2
c=$3
 expr $a + $b - $c
 expr $a - $b + $c
 expr $a / $b \* $c

To Print The Output We need to give Variable Values of a & b
./sample.sh 10 10

To Print The Output in Debug Mode command will be like 
sh -x sample.sh 10 10

LOCAL VARIABLE:
  #!/bin/bash
  month=jan
  touch fileA_$month                                                     a     b     c  ""

OUTPUT VARIABLE:
    
   memory=`free -m | grep Mem:|awk -F ' ' '{print $7}'`
   kernel=`uname -a|awk -F " " '{print $3}'`
   os=`cat /etc/os-release | grep ^NAME|awk -F '"' '{print $2}' | awk '{print $1}'`
   storage=`df -m /| grep -v Filesystem | awk '{print $4}'`
   
   
   echo "My Available MEMORY is $memory" 
   echo "My KERNEL_VERSION is $kernel"
   echo "My OS is $os"
   echo "MY STORAGE   is $storage                            

TOPIC:

Environment Variables: 
user level: Particular user Variables
global level: Entire system(any user can use them)

xargs: it prints the number of lines in a single line.
ifconfig eth0 |head -n2|xargs|awk '{print$1},"device ip address is",$6

>SEARCH BASH MAN PAGE
>IN THIS PAGE FIND CONDITIONAL EXPRESSIONS
>https://www.man7.org/linux/man-pages/man1/bash.1.html
--------------------------------------------------------------------------------------------------------------------------
#!/bin/bash
a=2
b=7
if [ $a -ge $b ]
then
  echo "The variable 'a' is greater than the variable 'b'."
else
  echo "The variable 'b' is greater than the variable 'a'."
fi
-----------------------------------------------------------------------------------------------------------------------------
if [ -d $1] 
then
 echo "$1 is a directory and it exists
else
 echo "$1 is not a directory and not exists"

fi

chmod +x filename.sh

./filename.sh /etc ---- It shows Whether there is a Directory or not

###############################################################################################################################
19/12/2022

More Examples on if Statements

#!/bin/bash

FILE=/RedSandal/Pushpa
if test -f "$FILE"; then
    echo "$FILE exists."
else
        echo "There is no such File, Creating New File..."
        mkdir /Aim
        touch Shoot
        mv Shoot /Aim
        echo "new file created and moved to /Aim Directory "
fi


####################################################################################################################################

(OR STATEMENTS)

#!/bin/bash
os=`cat /etc/os-release |grep ^NAME|awk -F '"' '{print $2}'|awk '{print $1}'` = Ubuntu

if [ $os = Amazon ] ||  [ $os = Redhat ] || [ $os = Centos ]

then

    echo "This is RedHat Family, so proceed with yum."

else

   echo "This is Not RedHat Family, So can't proceed further"

fi

#Whenever if you are using commands in shell script you should use `back quote`
########################################################################################
(&& STATEMENTS)

#!/bin/bash
os=`cat /etc/os-release |grep ^NAME|awk -F '"' '{print $2}'|awk '{print $1}'
kernel=`uname -a|awk -F " " '{print $3}'|cut -b -4` 

if [ $os = Amazon ] && [ $kernel -ge 5.11]

then
    echo "Your System is eligible for proceeding further"

else
    echo "Your system is not eligible for next step, Please EXIT."

fi

#######################################################################################################################
#!/bin/bash
user_id=`cat /etc/passwd|grep -i -w ^ec2-user|awk -F ':' '{print $3}'
echo "$1 user id is $user_id"

#########################################################################################################################
#PRINTING USER ID
#!/bin/bash
#START
User_Id=`id $1 | awk '{print $1}'`

echo " $1 userid is $User_Id"
##############################################################################################################################3
(elseif STATEMENTS)

#!/bin/bash

os=`cat etc/os-release |grep ^NAME|awk -F '"' '{print $2}|awk '{print $1}'`

#If 1st condition satisfies means it prints echo otherwise it will move to next
condition and checks wheteher it is satisfying or not.

if [ $os = Amazon]
then
  echo "This is Amazon"

elif [ $os = redhat ]
then
   echo "This is Redhat"

elif [ $os = centos]
then
   echo "This is centos"

fi
##########################################################################################
(for Loop STATEMENT)

#!/bin/bash
for username in `echo "$*"`
do
    user_id=`cat /etc/passwd| grep -i -w ^$1|awk -F ':' '{print $3}'`

       echo " $1 userid is $user_id"
done

#add username in the place of $1
###############################################################################################

#!/bin/sh
echo "Script Name: $0"
echo "First Parameter of the script is $1"
echo "The second Parameter is $2"
echo "The complete list of arguments is $@"
echo "Total Number of Parameters: $#"
echo "The process ID is $$"
echo "Exit code for the script: $?"

./ss.sh learning command line arguments

#################################################################################################

TODAY's TOPIC

#!/bin/bash
#START

set `date`
echo "day is $1"
echo "month is $2"
echo "date is $3"
echo "Time is H:M:S $4"
echo "Time Zone is $5"
echo "year is $6"

set -x

#END

####################################################

[root@devops /]# cat grp.sh
#!/bin/bash
#This is Task1
echo "1.Group AB"
echo "2.Group CD"
echo "Select Any one Option Above"
read -r ch
case $ch in
        1)echo "GROUP ID: `cat /etc/group|grep ^AB| awk -F ':' '{print $3}'` GROUP USERS: `cat /etc/group|grep ^AB| awk -F ':' '{print $4 $5 $6}'`" ;;
        2)echo "GROUP ID: `cat /etc/group|grep ^CD| awk -F ':' '{print $3}'` GROUP USERS: `cat /etc/group|grep ^CD| awk -F ':' '{print $4 $5 $6}'`" ;;
        *)echo "Invalid Option"

        esac
[root@devops /]# sh grp.sh
######################################################
#!/bin/bash
#This is Task2

opt=y
while [ $opt = y -o $opt = Y ]
do
        echo  "Please Enter the number: "
        read -r number 
        if [ $number -le 51 ]; then
                sq=`expr $number \* $number`       
                echo "square of provided number $number: $sq"
        else
                echo "number is not in given range"
        fi
        echo "Do you want to continue [Yes/No]: "
        read -r wish
        if [ $wish = yes -o  $wish = YES ]; then
                continue
        else
                echo "Thanks For Exiting.."
                exit
        fi
done
#############################################################
#!/bin/bash
for ((i=15; i>10; i++))
do
        echo $i
done
#################################################################################################
#!/bin/bash
#To Check Wheter The Application is Installed OR Not
#START
if ! [ -x "$(command -v jenkins)" ]; then
  echo 'Error: jenkins is not installed.' >&2
  else
          echo "jenkins is installed"
fi
######################################################################################
#!/bin/bash
#To Check Wheter The Application is Installed OR Not
#START
if which git >/dev/null; then
    echo exists
else
    echo does not exist
fi
##########################################################################################
#!/bin/bash
#To Check Wheter The Application is Installed OR Not
#START
if which httpd >/dev/null; then
    echo exists{Application is already installed}
else
    echo App is not installed hence installing...
    sudo yum install httpd
    sudo systemctl enable httpd
    sudo systemctl start httpd
   a=`sudo systemctl status httpd`
    echo "Application Status is $a"
fi
###############################################################################################
top command:

us: user cpu time (or) % CPU time spent in user space
sy: system cpu time (or) % CPU time spent in kernel space
ni: user nice cpu time (or) % CPU time spent on low priority processes
id: idle cpu time (or) % CPU time spent idle
wa: io wait cpu time (or) % CPU time spent in wait (on disk)
hi: hardware irq (or) % CPU time spent servicing/handling hardware interrupts
si: software irq (or) % CPU time spent servicing/handling software interrupts
st: steal time - - % CPU time in involuntary wait by virtual cpu while hypervisor is servicing another processor (or) % CPU time stolen from a virtual machine

Every program or task that runs on a computer system occupies a certain amount of processing time on the CPU. If the CPU has completed all tasks it is idle.

#######################################################################################################################################################################

CPU performance is one aspect of measuring the performance of a system, which is essential to measure the overall system-performance.

When a Linux system CPU is occupied by multiple processes, it is not available to process other requests, and the remaining pending requests must wait until the CPU is free.

If your system is under stress, it will slow down your application and becomes a bottleneck in the system.

There are numerous tools available to monitor and display CPU performance in Linux such as top, htop, glances, etc.

In this tutorial we have added two shell scripts to monitor the CPU utilization on Linux system, which is very useful when user has only few systems to monitor.



These scripts will trigger an email to the corresponding email id when the system reaches a given threshold.

#vi /opt/scripts/cpu-alert.sh

#!/bin/bash
cpuuse=$(cat /proc/loadavg | awk '{print $3}'|cut -f 1 -d ".")
if [ "$cpuuse" -ge 90 ]; then
SUBJECT="ATTENTION: CPU load is high on $(hostname) at $(date)"
MESSAGE="/tmp/Mail.out"
TO="gowraajay@gmail.com"
  echo "CPU current usage is: $cpuuse%" >> $MESSAGE
  echo "" >> $MESSAGE
  echo "+------------------------------------------------------------------+" >> $MESSAGE
  echo "Top 20 processes which consuming high CPU" >> $MESSAGE
  echo "+------------------------------------------------------------------+" >> $MESSAGE
  echo "$(top -bn1 | head -20)" >> $MESSAGE
  echo "" >> $MESSAGE
  echo "+------------------------------------------------------------------+" >> $MESSAGE
  echo "Top 10 Processes which consuming high CPU using the ps command" >> $MESSAGE
  echo "+------------------------------------------------------------------+" >> $MESSAGE
  echo "$(ps -eo pcpu,pid,user,args | sort -k 1 -r | head -10)" >> $MESSAGE
  mail -s "$SUBJECT" "$TO" < $MESSAGE
  rm /tmp/Mail.out
else
echo "Server CPU usage is in under threshold"
  fi

cat /proc/loadavg : 0.02 0.02 0.00 1/169 3979
 The first three columns measure CPU and IO utilization of the last one, five, and 10 minute periods. The fourth column shows the number of currently running processes and the total number of processes. The last column displays the last process ID used.

https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/4/html/reference_guide/s2-proc-locks
#########################################################################################################################
#!/bin/bash
# This script monitors CPU and memory usage

while :
do 
  # Get the current usage of CPU and memory
  cpuUsage=$(top -bn1 | awk '/Cpu/ { print $2}')
  memUsage=$(free -m | awk '/Mem/{print $3}')

  # Print the usage
  echo "CPU Usage: $cpuUsage%"
  echo "Memory Usage: $memUsage MB"
 
  # Sleep for 1 second
  sleep 1
done

########################################################################################
----FOR LOOP

a=`expr$a+1`
a=2

a=0
# -lt is less than operator

#Iterate the loop until a less than 10
while [ $a -lt 10 ]
do
	# Print the values
	echo $a
	
	# increment the value
	a=`expr $a + 1`
done

########################################################################################
#!/bin/bash
COLORS=" RED BLUE GREEN YELLOW BLACK "
#iterate the for loop until total values printed

for COLOR in $COLORS
do
    echo "COLOR: $COLOR"
done

#######################################################################################################################################################################
i=1
for day in mon tue wed thu fri sat sun
do
echo -n "Day $((i++)) : $day"    = day1 : mon         day2:tue
if  [ $i -eq 7 -o $i -eq 8 ];
then
 echo "$day is WEEKEND"
 
 continue;
 fi
  echo "weekdays"
  done
#########################################################################################################################################################################
                                                                            BACKUP  FILES IN LINUX
#########################################################################################################################################################################
vi backup.sh

#!/bin/bash
tar -cvf /tmp/backup.tar /etc/tar
gzip /tmp/backup.tar
find /tmp/backup.tar.gz -mtime -1 -type f -print &> /dev/null
if [ $? -eq 0 ]
then
echo "backup was crearted"
echo " Archiving files "
#scp /tmp/bacup.tar.gz root@:ip:/path
else
echo "Backup files"
fi
___________________________________________________________________________________________________________________________________________________________________________
How to Create and Manage Cron Jobs on Linux
Cron is one of Linux’s most useful tools and a developer favorite because it allows you to run automated commands at specific periods, dates, and intervals using both general-purpose and task-specific scripts. Given that description, you can imagine how system admins use it to automate backup tasks, directory cleaning, notifications, etc.

Cron jobs run in the background and constantly check the /etc/crontab file, and the /etc/cron.*/ and /var/spool/cron/ directories. The cron files are not supposed to be edited directly and each user has a unique crontab.

$ crontab -e

#Install crontab
yum install cronie

Cron Syntax:

A B C D E USERNAME /path/to/command arg1 arg2
OR
A B C D E USERNAME /root/backup.sh

15 03 22 * * root /root/backup.sh

Explanation of above cron syntax:

A: Minutes range: 0 – 59
B: Hours range: 0 – 23
C: Days range: 0 – 31
D: Months range: 0 – 12
E: Days of the week range: 0 – 7. Starting from Monday, 0 or 7 represents Sunday
USERNAME: replace this with your username
/path/to/command – The name of the script or command you want to schedule

that’s not all. Cron uses 3 operator symbols which allow you to specify multiple values in a field:

Asterisk (*): specifies all possible values for a field
The comma (,): specifies a list of values
Dash (-): specifies a range of values
Separator (/): specifies a step value

Cron Job Examples
The first step to running cron commands is installing your crontab with the command:
# crontab -e

Cron Job Realtime EXample:
*/1 * * * * /root/shell.sh >> /root/ashish.txt

Run script.sh at 3 am every day:
30 16 2 * * /path/to/script.sh

Run /scripts/phpscript.php at 10 pm during the week:
0 22 * * 1-5 /scripts/phpscript.php

List cron jobs.
# crontab -l
OR
# crontab -u username -l

Delete all crontab jobs.
# crontab -r

Delete Cron job for a specific user.
# crontab -r -u username

How to see Users crontab
# crontab -u username -l
##############################################################################################
https://www.javatpoint.com/while-loop-shell-scripting

#############################################################################

How to connect to Server To Server

let me try like this:
There are 2 Servers called ServerA & ServerB
ifconfig command in Server B
#ssh root@privateip in ServerA -- YOU WILL FAIL

By default Passwordless authentication is disabled, we should enable the password.
IN SERVER DO BELOW:
#vi /etc/ssh/sshd_config
#systemctl restart sshd
CREATE PASSWD TO ROOT USER
Firstly login as root user & give passwd
Now, Do the same like as we did in firsttime
#ssh root@privateip in ServerA
now it will ask passwd & give passwd

2nd WAY..
--now, ServerA
#ssh-keygen
#cd /root/.ssh
#ls
#cat id_rsa.pub {copy the key here}
--now, ServerB
#cd /root/.ssh/
#ls
#vi authorized_keys {paste the key here}

finally, do ssh privateip of ServerB

##################################################################################################################################
                                                         AWS
##################################################################################################################################
IAAS: Infrastructure as a service.
PAAS: Platform As A Service
SAAS: Software As A Service 

How To Launch AWS INSTANCE::
1)CHOOSE AN AMI-- Amazon Machine Image
2)Instance Types-- a)General Purpose b)CPU Optimize c)Storage Optimize D)Memory Optimize
https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-types.html
3)Configure Instance Details-- a)Network b)Instance Count c)Iam d)Advanced Settings e)storage f)tags g)security groups

###########################################################################
IP ADDRESS:
1)STATIC
2)DYNAMIC
3)PRIVATE  eg: 17.10.0.0
4)PUBLIC   eg: 255.254.254.254/0
How to Calculate IP ADDRESSES:
192.124.11.01

calculate IP Address:

####################################################################################
IAM USERS,POLICIES,ROLES-------
CUSTOM POLICIES----
Suppose i don't want to give S3 Bucket Full Access. I want to give permissions only to particular S3 Bucket only. Here, CUSTOM POLICY comes into picture.
---select create "custom policy"
---select visual editor (or) Json
---select service eg: S3
---select Actions
---Access Level {Select any of those}
---Review Policy {Give the name and desp}

@Managed Policy Created
---select "Inline policy" {That policy is created to particular user only and it is not added to policies}
---select visual editor or Json
---select Service eg:S3
---select Actions {eg: List Bucket}
---Resources Will get enable here
---Add arn {give bucket name} (arn:aws:s3:::bucket_name)
---Review Policy
@Inline Policy Created

PERMISSION BOUNDARIES----
Suppose Particular user is having AdmicFullAccess and you want to restrict or set a boundary for him in using Admin_Full_Access.
---Permission Policies {It shows how many permissions are addded to a user Like i.e.,Authorization/Access}
---Permission Boundaries
---select Set Boundary
---select policy {s3 bucket}
###############################################################################################################################

ELASTIC BLOCK STORAGE(EBS)----
Suppose You are not having enough storage and you need some more. what will you do, just simply add EBS and mount it to your server.
---Create New Volume with desired Size value in EBS
---Make sure the EBS Volume and instances are in the same zone
---Select the created volume, right click & select "attach volume" option
---Select the ec2 Instances in the instance box
---now login to your instnace and list available disks using the command "lsblk"
---check if the volume has any data using the "sudo file -s /dev/xvdf"  {/dev/xvdf: data}
---format the volume to the "ext4" filesystem using the "sudo mkfs -t ext4 /dev/xvdf or sudo mkfs -t xfs /dev/xvdf"
---now create a directory of your choice to mount our new ext4 volume. "sudo mkdir /Directory_Name"
---Mount the volume to Directory_Name using "sudo mount /dev/xvdf /Directory_Name"
---Do, df -h
###################################################################################################################################

ELASTIC FILE SYSTEM(EFS)----
one Petabyte is equal to 1,000 Terabytes
---Search EFS in Search Bar
---Select create EFS
---You can attach EFS to Ec2-Instance, Hit Attach Button
---You can attach it in two ways
--- 1.Mount via DNS 2.Mount via IP
---https://docs.aws.amazon.com/efs/latest/ug/mounting-fs-mount-helper-ec2-linux.html

---> How do I control which Amazon EC2 instances can access my file system?

You control which EC2 instances can access your file system using VPC security group rules and IAM policies. Use VPC security groups to control the network traffic to and from your file system. Attach an IAM policy to your file system to control which clients can mount your file system and with what permissions, and use EFS Access Points to manage application access. Control access to files and directories with POSIX-compliant user and group-level permissions.
####################################################################################################################################

SIMPLE STORAGE SERVICE(S3)----
Its all started on 2004. It is first started with Simple Use Service(SUS). S3 released on March 2006 got popularity.
1.Scalability: You can store huge data upto PetaBytes starting with 0.001bytes. You Need to pay only for the storage that you have used. 
2.Availability: Suppose you have pendrive and in that drive you have one copy of particular data and you loosed data due to pendrive corrupted or virus, you can't get it back.

 Coming to s3 It Stores your data in three copies Bcoz you have issue with data center.
3.Internet: Only internet is enough to retrieve your data whenever you required

---Search S3 in Search Bar
---Create Bucket (You need Provide Globally Unique name to your bucket, it won't accept duplicate names)
---Select AWS region (ap-south-1)
---Create Bucket
@Bucket_created
---You can Upload the files Using "UPLOAD" Tab
---The Files that are stored in Bucket are called "OBJECTS"
---You can Download The files using "DOWNLOAD" Tab
---You can Create Folders in Bucket 
---You can give access on your files to other users publicly by generating URL
---There are 2 Types of Websites 1.DYNAMIC WEBISTE 2.STATIC WEBSITE
---Select any Object 
---By default the URL is in private mode and you can't access the url
---Select "Object Actions" and select "Make Public"
---Before Make it as Public, Make some configurations in Bucket Permissions.
---You will Find BLOCK_PUBLIC_ACCESS, Edit it
---Save Changes
@NOW_DATA_IS_PUBLICLY_ACCESSED

S3BUCKET_STORAGE_CLASSES----
---Take a Particular Region #ap-south-1
---Using S3 Bucket, You can store the data in AZ1
---Due to Un-Predictable Conditions like short circuit, earthquakes. You lost the data in that AZ
---You will raise a request to AWS for your data. It will check in AZs now 
---Storage Classes: 1)Standard Storage 2)Reduced Redundant Storage 3) Intelligent Tier 4)Standard Storage- IA 5) One Zone-IA 6)Glacier 7)Glacier Deep 
--- https://aws.amazon.com/s3/storage-classes/
---Four Parts 1)Frequently Accessed 2)In-frequently 3)Archival
---During Files upload you can give the Access Control List Permissions, Now you can choose Grant Public Read Access
---If you select the Properties. There you can select the storage classes. By Default Standard calss is selected.

CREATING_BUCKET_POLICY----
Bucket Policy is to giving permissions to each and every file in the bucket for public access
---Go To Permissions
---Bucket Policy
---Select Edit
---Select Policy Generator
---Principal {If you want to give Permission to all users give *}
---Actions {If you want to give All actions SELECT ALL ACTIONS}
---Bucket ARN {Copy the bucket ARN & Paste it over Here}
---Click On Generate Policy
---You will get Json Format and copy and paste it in S3 Console
---At Resource add /* to apply it for all files for public Access, Give Inside the " qutotation.

(To Inspect The Page. Right Click On your Mouse Select Inspect)
 
ACCESSING_PRIVATE_OBJECT_PRESIGNED_URL----
---Delete The Bucket Policy
---Login to your server and give Public Access Key and secret key
---Go to your name on top most right hand corner, Click on that
---Select security Credntials
---Generate Credentials at access keys
---Go to your instance and give "aws configure" command
---Paste the access keys and Secret Keys & also give the Region Name {AKIAYHTJR3DZ6H4FOEHK       L+tEGTLH5ZAXEY4fXE8DNyyy5UicscyI7tHpzW0e}
---Search AWS S3 CLI COMMANDS in Browser and search for Presign Command
---Now, in CLI give aws s3 presign and S3 URI
---Give Expiration Time using --expires-in (eg. 60secs)
---It will generate URL and u can try in web.

S3_BUCKET_ENCRYPTION----
---The Files that are stored in S3 Bucket are not encrytpted by default
---If you want to Encrypt the files. Just do the following 
--- Go to properties and make changes
---Static Website is disabed, Enable Itsss
---Enable The encryption as well

Copying files from EC2 to S3 is called Uploading the file

---> If your server is ubuntu OS, you need to install aws cli
     $ apt  install awscli

---> $ aws configure
Access Key: AKIAYHTJR3DZ6WNOAIWZ
Secret Access Key: ydjuYh/AsQiflrz0L+yeKlQYJ9qqTLz3FcVBOG6j


# To List the S3 Bucket 
aws s3 ls s3://<S3bucketName>

# To copy the files from EC2 to S3
aws s3 cp <Fully Qualified Local filename> s3://<S3BucketName>

Copying files from S3 to EC2 is called Downloading the files

# To copy the files from S3 to EC2
aws s3 cp s3://<S3BucketName> <Fully Qualified Local filename/Directory>
#####################################################################################################################################

SNAPSHOT----
We are attaching EBS Volume to EC2 Instances and we have data in EBS Volume. Suppose if we lost the data in EBS Volume, we can create Snapshot for Backup of Data. The Data that is stored in EBS Volume will be copied to snapshot. The Snapshots are stored in S3 Buckets in Backend. The Aws will hide that buckets for security purposes. They Will Call it as "OPAQUE" S3 Buckets. 
E.g: If Your Server is having attached to 10 Number of volumes and you need to create snapshot each and every volume. You can't create Snapshots at a time.

Charges: Only Charged to Snapshots

Amazon Machine Image(AMI)----
Actually What is AMI, the system configurations along with the applications installed on that server. You can create Image to you server. It is like backup to your server. For instance your server is crashed due to overload issues. Here with the help of AMI you can backup your server. You are generally creating servers with custom AMIs.

  AMI will create a template. What Template will do it will stores the OS, Installed applications with versions, Hosted Servers, Settings and also create a snapshot for all the volumes. AMI Means Template + Snapshot of EBS volumes. Now, You don't require extra configuration and Maintainence.

Charges: Only Charged to Snapshots and No charges to AMI Templates.
Costing: Charges are levies only on used storage of snapshot.
For eg: You have taken 100Gib of EBS but you have used only 50Gib only. You will get charges of 50Gib.
---Snapshots should be always lessthan or equal to EBS volumes.

---@@@CREATING SNAPSHOTS
---Go To Elastic Block Storage
---Select Volumes
---Select Volume that you want to attach SNAPSHOT
---Click on Actions
---Click on Create Snapshot
---Here, Give Description and Tags
@@@SNAPSHOT CREATED.

PORT NUMBERS----
---A port number is a way to identify a specific process to which an internet or other network message is to be forwarded when it arrives at a server. All network-connected devices come equipped with standardized ports that have an assigned number.

############################################################################################################################
https://www.cloudflare.com/learning/network-layer/what-is-a-computer-port/
############################################################################################################################

Elastic Load Balancer(ELB)---- {HIGH AVAILABILITY, Health Checks}

---Before Elastic Load Balancer what we do. If your hosting any website, You should have a server for your website and there is IP address for that server and you are attaching IP Address to domain name of your website. Whenever user wants to connect to your website they can simply search the Domain Name of your webiste that will redirect to your server.

---Elastic Load Balancer will map the IP Addresses of all Servers to Domain Name of Website called "CNAME".

---Elastic Load Balancer is also like a software which is installed in server, it may not bare number of requests made by user. it also requires auto scaling.

--- Most Important Pre-Requisite of ELB is should have Two Subnets to create a Load Balancer. Load Balancer is not created in single availability Zone. Suppose you are having 2servers in AZ1 and you wants to create a loadbalancer means it won't create. 

--- Open System Interface Model. It is created by Inetrantional Standards Organization. There will be 7 Layers in this OSI Model. 
--- 1.Application Layer
--- 2.Presentation Layer
--- 3.Session Layer
--- 4.Transport Layer
--- 5.Network Layer
--- 6.DataLink Layer
--- 7.Physical Layer

######################################################################################################################################
Vertual Private Cloud
Tiers:
1 Tier - WEB Tier, APP Tier, DATABASE Tier Altogether in one Place
2 Tier - WEB Tier, APP Tier at one Place
3 Tier - All these THREE are in different places.
######################################################################################################################################

---@@@ HOW TO CREATE ELASTIC_LOAD_BALANCER @@@---
---Search Elastic_LOAD_BALANCER             (OR)
---Search LOAD_BALANCING in left side pane
---First CREATE TARGET_GROUPS
---Give TARGET_GROUP_NAME
---Select TARGET_TYPE
---Select PROTOCOL
---PORT
---Select Desired VPC
---###ADVANCED_SET_HEALTH_CHECK_SETTINGS
---PORT
---Set Healthy_Threshold
---Set UnHealthy_Threshold
---Set Timeout
---Set Interval
---Set Success_codes
@@@TARGET_GROUPS is Created
---Select_Applcation_LoadBalancer
---Give Name of LoadBalancer
---Scheme
---Set IP_Address_Type
---Set Availability_Zones

#####################################################################################################################################

ICANN- INTERNATIONAL CORPORATION FOR ASSIGNING NAME AND NUMBERS: 
Numbers Matalab- IPAddresses
Names Matalab- Domain Names
There are number of Domain Name providers registered under ICANN Called "DOMAIN_REGISTRARS"
E.g: Amazon Registrar
TLD: Top Level Domains e.g: .com, .in, .gov, .org, .edu    www.ajay.in
Domain Registry: This will provide TLD. 
Domain Name Portability:You can change The Backend provider of the Domain Name. Suppose you have taken Domain Name in Route53 and you can port it to other service Providers like Godaddy, azure, GCP {Feasibility}

DNS Providers:

Cloudflare DNS.
Amazon Route 53.
DNS Manager.
GoDaddy Premium DNS.
BloxOne DDI.
NS1.
ClouDNS.
DNSMadeEasy.

Route53----
--- If a website needs a name, Route 53 Registers the name for the website(Domain Name)
--- Route53 helps to connect the browser with the website or web application when the user enter the domain name
--- Route53 checks health of resources by sending automated requests over the inetrnet to a resource
--- Ensures a consistent ability to Route the applications {Highly Reliable}
--- Automatically handles large queries without the user's interaction
--- Easy to sign up, configure DNS settings and provides facts response to queries
--- Pay only for the service used {Cost Effective}
--- Only Authorized users can access the Route53 Like IAM Users {Security}
####################################################################################################################################
VERTUAL PRIVATE CLOUD----
A virtual private cloud (VPC) is a secure, isolated private cloud hosted within a public cloud. VPC customers can run code, store data, host websites, and do anything else they could do in an ordinary private cloud, but the private cloud is hosted remotely by a public cloud provider. (Not all private clouds are hosted in this fashion.) VPCs combine the scalability and convenience of public cloud computing with the data isolation of private cloud computing.

Imagine a public cloud as a crowded restaurant, and a virtual private cloud as a reserved table in that crowded restaurant. Even though the restaurant is full of people, a table with a "Reserved" sign on it can only be accessed by the party who made the reservation. Similarly, a public cloud is crowded with various cloud customers accessing computing resources – but a VPC reserves some of those resources for use by only one customer. 

NAT GATEWAY: How to Downlaod Files, Update the Files, Install the Application in Web Server, App Server and DB Server Private Subnet. Here NAT Gateway Comes into Picture, NAT Gateways are created in Public Subnets and is attached to the route table of Private Subnet. Here unique Feature of NAT Gateway is it permits the other users in using the private servers. No requests will be shared to Private Servers since NAT Gateway not allows it. There is no Two Way Traffic. Only One Way Traffic is possible. Whenever the owner or Admin

INTERNET GATEWAY: Internet Gateway allows Two way connection which helps to connect to internet. Ingress ntework which allows the traffic inside the VPC. Egress Network which allows the traffic to Outside.

NACL(Network Access Control List): NACL Works as Security Firewalls to Subnets. Each subnets will have their own NACLs. You can configure the NACLs to give permissions to the users who can be able to enter into the Subnets.

SECURITY GROUPS: SecurityGroups Works on servers in the network. You can open the port numbers in Security Groups and you can define who can access that port numbers

VPC FLOWLOGS: If you want to know the request sending by you is passing through from which Server to which server. You can simply enable the flow logs bye default it is disabled but you can enable it. It stores the records. 

---> Class Index Domain Range: ClassA, ClassB, ClassC, ClassD, ClassE
ClassA Range: 0.0.0.0 to 126.0.0.0
ClassB Range: 127.0.0.0 to 192.0.0.0
ClassC Range: 193.0.0.0 to 223.0.0.0
ClassD Range: 224.0.0.0 to 239.0.0.0
ClassE Range: 240.0.0.0 to 255.0.0.0

09/01/2023..
How to create VPC in AWS:
---STEP:1 Search For VPC in Search Bar
---STEP:2 Click on your VPC
---STEP:3 Select Create VPC
---STEP:4 Give some Name for VPC
---STEP:5 Give IPv4 CIDR Block range {10.0.0.0/16}
---STEP:6 Keep Tenancy as a Default One
---STEP:7 Create VPC

Creating Subnets:
---STEP:1 In the left side pane of the VPC DashBoard, U can select Subnets
---STEP:2 Click on create Subnets
---STEP:3 Select Your VPC to attach subnets
---STEP:4 Give name of Subnet
---STEP:5 Select a AZ {Availability Zone}
---STEP:6 Give CIDR Block Range to subnet {10.0.0.0/24}
---STEP:7 Click add another subnet
---STEP:8 Same as previous 4,5,6 Steps.

Creating Route Tables
---STEP:1 Select Routetables in VPC Dashboard 
---STEP:2 Create RouteTable
---STEP:3 Name Router
---STEP:4 Associate Subnet

Create Internetgateway
---STEP:1 Select Internet Gateway in VPC Dashboard
---STEP:2 Create InternetGateway and attach it to VPC
---STEP:3 Go To Private/public Route Table
---STEP:4 Edit Routes
---STEP:5 Add Internet 0.0.0.0 and select target Internetgateway

HOW TO CONNECT PRIVATE SERVER USING PUBLIC SERVER
---STEP:1 LOGIN TO YOUR PUBLIC SERVER
---STEP:2 ADD PEM KEY FILE By using vi editor {vi private.pem}
---STEP:3 chmod 400 private.pem
---STEP:4 ssh -i private.pem private DNS

CREATION OF NAT GATEWAY{Network Address Translation}
---STEP:1 Select NAT GATEWAY from VPC Dashboard
---STEP:2 Choose Create NAT GATEWAY 
---STEP:3 Give name
---STEP:4 Select Public Subnet {Why Not Private Subnet means, If you place NAT Gateway in Private subnet means the internet that is allowed to NAT Gateway leads to others can also enter into private subnet, it leads to data hacking by hackers }
---STEP:5 Select Connectivity Type Public {if company is having on premises servers or their own data cenetrs can select that one}
---STEP:6 ALLOCATE ELASTIC IP {It creates ELASTIC IP}
---STEP:7 Attach NAT GATEWAY to Private Subnet Edit the Route.

AUTO SCALING:
---Horizontal Scaling means it scales the servers horizontally. It means it increase the servers with same configurations like 2core and 4GB Ram for all servers.
---Vertical Scaling means like if you want to improve the server capacity to only one server by eliminating the other previous server
---Drawback of vertical scaling is downtime.
---If the application is in different subnets in different availability zone, you should create separate loadbalancer.
--- DNS Name may also different for that application or website
---Instead of creating different loadbalancers. we can create a loadbalancer for DNS Name

CREATING AUTOSCALING:
---Create VPC
---Create Subnets
---Create Internet Gateway
---Check the RouteTable for each Subnet
---Create LoadBalancer
---Select application loadbalancer
---Give the name of the LoadBalancer
---Select the scheme- 1.InternetFacing 2.Internal {if you want for Private Load balancer}
---Select Default listner e.g: HTTPS
---Select the VPC {Needs to select atleast 2 Availabilty zones}
---Create Security Group, Give name of SG and open the port number 80 fot HTTP
---Configure Routing
---Give the Target Group Name and Target Type {Instance}
---Health Checks {Give the path of application file}
---You can also check the DNS of loadbalancer in DNS Health Checker in the web browser

---cookiecutter
---Launch Normal Instance
---Choose VPC of autoscaling Group
---Select public Subnet
---Open Port Number 80 in Securitty Group
---Create a new key pair
---launch instance
---Login to instance
---Install httpd in the server
---create a directory in /var/www/html e.g: /app
---create index.html file in /app directory
---if you want to start the service of httpd whenever the server is On {chkconfig httpd on}

---Now, Go to Your AWS Console and select your instance, click on actions
--- Select Create image
---Give the image name and save it.

----AutoScaling Launch Configuration
---Name the Launch Configuration
---Select the instance type
---Select the AMI
---Select the Security group and name it
---Open the port number 80
---Select/Create the key pair here.

---Create AutoScaling Group
---Give Autoscaling Name
---In Launch Configuration Template choose Launch Configuration that you have created
---select VPC
---Select subnet
---Attach from existing loadbalancer
---In Health Checks select ELB
---Grace Period {Suppose if your CPU Utilization is reached the threshold value like maximum Utilizatiob the auto scaling wait for somtime to check whether that cpu utilization is optimized or not.}
---Here we need to give graceperiod time
---Group Sizing
---Desired Capacity {When you are launching autoscaling group how many instnaces you want}
---Minimum Capacity
---Max Capacity
---Scaling Policies
---Select Tracking Scaling Policy
---Give the policy Nmae
---Metric type {Average CPU Utilization}
---Give Target Value ()
---Don't select disable scale in to create only a scale out policy

---Now, Try to create Target Group for application2
---Select the protocol number 80 to listen from HTTP
---Select the VPC
---Select the Health Checks and give the path of application2

CLOUDWATCH ALARM----
---Cloudwatch monitors most of the Services in AmazonWebServices
---Some of Monitoring Tools are 1.Nagios, 2.Splunk, 3.Jabix, 4.DataDog
---CPU Utilization
   Memroy Utilization
   Harddisk
STATUS CHECKS: Your EC2 Checks should be - 2/2 {If it is 1/2 you can't able to connect your Server}
---> System status checks
---> Instance status checks
---In ec2 instance select your server
---In that please select Status Checks
---Select Actions
---In That Select status checks Alarms
---Add or Edit Alarams {Create a New Alarams}
---Add Simple Notification Service{SNS}

* How To Create Cloud Watch Alarms

---> To create an alarm using the Amazon EC2 console
Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/.

---> In the navigation pane, choose Instances.

---> Select the instance and choose Actions, Monitor and troubleshoot, Manage CloudWatch alarms.

---> On the Manage CloudWatch alarms detail page, under Add or edit alarm, select Create an alarm.

---> For Alarm notification, choose whether to turn the toggle on or off to configure Amazon Simple Notification Service (Amazon SNS) notifications. Enter an existing Amazon SNS topic or enter a name to create a new topic.

---> For Alarm action, choose whether to turn the toggle on or off to specify an action to take when the alarm is triggered. Select an action from the dropdown.

---> For Alarm thresholds, select the metric and criteria for the alarm. For example, you can leave the default settings for Group samples by (Average) and Type of data to sample (CPU utilization). For Alarm when, choose >= and enter 0.80. For Consecutive period, enter 1. For Period, select 5 minutes.

(Optional) For Sample metric data, choose Add to dashboard.

Choose Create.
---First of all create SNS
---Just follow the link pasted Below
---https://docs.aws.amazon.com/sns/latest/dg/sns-getting-started.html	
---Follow the Simple Steps below to create Cloud Watch alarm.  
---https://docs.aws.amazon.com/sns/latest/dg/sns-getting-started.html#step-receive-delete-message	

Basic Monitoring: It comes by by default
                  Free of Cost
                  Every 5 minutes

Autoscaling--- Scaling Policies

############################################################################################################################           
                                          ----VERSION CONTROL IN SOFTWARE----
############################################################################################################################
---Let's imagine there's a Multinational Company which is having offices and employess all around the globe
---There are number of challenges the company may face like collaboration. there are set of people working in the same project in different regions
---The other challenge may be like storing version the project is not completed with a single version. There may be n number of versions. The problem here is stroing the commits in a single place is big challenege.
---The another challenge is Restoring Previous Versions. It is very important to store the previous version. why because suppose a bit of code was updated in previous code but is having bugs leads to application working issues on that time developer needs to call back the previous code
---Backup: It is also a kind of challenge where developer doesn't have backup disk and it he lost the data means, he can't get back the things that he lost. All efforts may go into vein.
---All these problems can be solved with the help of a "Version Control System".
---The version control system will take care of developers collaboration and also stores the previous versions. Here, Whenever the developer can rollback to previous versions.

---BENEFITS OF VERSION CONTROL: Helps and managing source code and protecting the source code
---Keep Track of all the Modifications made of the code. Like who modified the code, who made changes in the code date, time and all.
---Comparing earlier versions of the code.
---Best Version Control System: GitHub, GitLab, Perforce, Beanstalk, AWS Code, BitBucket, Tortorise.

INTRODUCTION TO GITHUB----
---Git is the most popular version control system.
---First of all let us know about what is push and pull.
---Whenever the Developer commits the code into the repository is called push.
---Whenever the Developer or any other person pull the code from the repository is called pull.
---There are two Different Types of Repos
---CENTRALISED REPO: Let us take an example two developers are working on versions like v1 & v2 which are stored in centralised repo. suppose if you lost the internet connection. You can't pull the code from the repository.
---DISTRIBUTED REPO: Here we take another example like two developers are working on versions like v1 and v2 which is stored in server and also in local repository. The version is stored in his hard drive and even there is no Internet Connection. They can work on their code.

----Alternative Tools for Source code Management tool: "Git, GitLab, Bit Bucket, Mercurial, Subversion" 

GITHUB ACCOUNT CREATION----
---Search GITHUB Account signup in your Web Browser
---Choose always official site.
---Give your Mail ID & Create Password (Username also), Click on Continue Button.
---You need to verify your gmail that you have given and create your account.
---Install Git on your server.
---You can clone the git on your server, just copy the repo web address in git and paste it in your server.
---git clone https://github.com/organisationName/reponame.git
---You can create files and directories from your command line interface itself and push it to the github


Git Merge Conflicts:: If Two Devlopers modifies same file in different branches and if they try to merge those branches is called git merge conflicts.

GIT COMMANDS----
---git init {This command is used to create a empty repository locally}

---git add . (or) git add filename

---git rm filename {It removes file or directory}

---git commit -m "example messgae"

---git push
---Here it will asks your mail id and passwd{In the place of passwd you need to give github token which can be generated in github}

---> How to Generate the Token settings/DevloperSettings/personalaccesstoken/select repo click on Generate token.

---git checkout branchname {To swtch between braches}

---git pull {To pull the updates from git repository}

---git branch -a {it shows number of branches which are created in remote and Local repository}

---git fetch is the command that tells your local git to retrieve the latest meta-data info from the original (yet doesn't do any file transferring. It's more like just checking to see if there are any changes available). git pull on the other hand does that AND brings (copy) those changes from the remote repository.

---You can pass 2commands in one shot { git add. ; git commit -m "committing new chnages"}

---git log {You can check the commits that you have done so far and you can see who made commits at what time and commit message using.}

---git show Commit_id {it shows you full details of commit}

---git revert Commit_Id {If you want revert back the commit that you have did previously, you can use this command}

---Git Architecture Explanation
****   When you made some changes in Working Directory of Local Repo the Flow is like below
   Do some changes in a file in Working Directory and give Command {GIT STATUS} {Output: file Modified in Red Color} [UN-TRACKED FILE]
   Now Check the status of changes with Command {GIT ADD .} {Output: file Modified in Green Color}
   If You want some files to be untracked, give command {GIT RESTORE --STAGED FILE_NAME}
   Now Give the Command {GIT COMMIT -M "    "} ,It will be in Unmodified stage. Changes of file will move from working Directory to Local Repo
   Now Do {GIT PUSH}, it will push the changed file to Remote Repo
****   When You made Some Changes in Remote Repository and wants to see changes in local do follow below
   Give {GIT PULL} Command
--- CENTRAL REPO: For Example There are 3 developers working on same project and they are developing different features of application and push the code to central repo, there they can integrate the code.   

---> .git folder contains the Dependcies of git, binaries, source code all the things in .git directory
---> IGNORING CONTENT: It will be useful when you don't want to track some specific files then we use a file called (.gitignore)
---> vi .gitignore >> filename >> save the file.
---Some More Commands on GIT BRANCH 
---To Add New Branch: git Branch Branch-name
---To Create and Switch at a Time: git checkout -b branchname
---To Rename a Branch: git branch -m Old New NewName
---To Delete a Branch: git branch -d <branch> (-D force delete) Rule: You Can't delete the current active branch, checkout from it to delete branch.
---Suppose assume you are a developer and you have made changes in particular file. Unfortunately you made a change in unwanted file and now you dont want push it but you want to save it, now do the following:
              git add filename
              git stash
--- If you completed the changes on target file and you want to push the file 
              git stash apply 
--- If you want to get the changes happend in particular branch and you want get the changes in another branch use the command.
              git cherry-pick commitid
---https://www.atlassian.com/git/tutorials/using-branches/git-merge

---> Git Merge Example

     $ git checkout master

     Create a new branch based on master:

     $ git branch feature

     $ git checkout feature

    Adding the footer file:

     $ git add footer.php

    Now, commit the changes:

    $ git commit –m “added footer component”

   After the work is done for adding the footer component, you may merge it into the master branch as follows:

    $ git checkout master

    $ git merge feature

  The above command should merge the feature branch commits into the master branch, so both branches are now at the same level.

What Is Git Rebase?
Rebase is one of two Git utilities designed to integrate changes from one branch onto another. Rebasing is the process of combining or moving a sequence of commits on top of a new base commit. Git rebase is the linear process of merging.

What Does Git Rebase Do?
A Git rebase changes the base of the developer’s branch from one commit to another, so it looks like they have created their branch from a different commit. Internally, Git creates a new commit and applies it to the specified base. However, it's essential for everyone involved to understand that although the branch appears the same, it's made up of entirely new commits. When you perform a Git rebase, you are, in effect, rewriting history.

Here’s how Git rebasing compares to Git merging. Let's say you're a developer who is working on a new feature on a dedicated branch. Then, another development team member updates the main branch with some new commits. The situation looks like this:

Git_Rebase_1.

Eventually, however, the team concludes that the main's new commits are relevant to the feature you are working on. So then, if you want to incorporate the new commits onto your branch, you can either do a merge or a rebase. If you decide to use Git merging, you tack on the new commits to your new branch like this:

Git_Rebase_2.

However, if you use Git rebase, you move your whole feature branch, starting it on the tip of the main branch so that all the new commits are now part of the whole. This action rewrites the project history by making new commits for each of the original branch's commits.

What is Git Rebase, and How Do You Use It?
By John Terra
Last updated on Feb 24, 2023127078

What Is Git Rebase, and How Do You Use It?
Table of Contents

What Is Git Rebase?What Does Git Rebase Do?What Is Git Rebase: Git Rebase UsageGit Rebase Standard vs. Git Rebase InteractiveHow to Git RebaseView More
Developers today face an ever-increasing demand for more applications. Consequently, developers must ensure they have the best tools for the job. The DevOps design methodology has a good collection of tools and resources for the developer, including Git.

Git is an open-source version control system often used for source code management. It features a plethora of commands and functions that make the developer’s job easier. That’s why today we’re here to discuss the Git rebase command.

This article provides a deep dive into rebase in Git. We’ll explore what Git rebase is, what it does, and how to use it. We will also cover other related concepts such as Git rebase branch, Git merge rebase, and Git pull rebase.

So, let's start off with the question, "What is Git rebase?"

Learn from the Best in the Industry!
Caltech PGP Full Stack DevelopmentEXPLORE PROGRAMLearn from the Best in the Industry!
What Is Git Rebase?
Rebase is one of two Git utilities designed to integrate changes from one branch onto another. Rebasing is the process of combining or moving a sequence of commits on top of a new base commit. Git rebase is the linear process of merging.

What Does Git Rebase Do?
A Git rebase changes the base of the developer’s branch from one commit to another, so it looks like they have created their branch from a different commit. Internally, Git creates a new commit and applies it to the specified base. However, it's essential for everyone involved to understand that although the branch appears the same, it's made up of entirely new commits. When you perform a Git rebase, you are, in effect, rewriting history.

Here’s how Git rebasing compares to Git merging. Let's say you're a developer who is working on a new feature on a dedicated branch. Then, another development team member updates the main branch with some new commits. The situation looks like this:

Git_Rebase_1.

Eventually, however, the team concludes that the main's new commits are relevant to the feature you are working on. So then, if you want to incorporate the new commits onto your branch, you can either do a merge or a rebase. If you decide to use Git merging, you tack on the new commits to your new branch like this:

Git_Rebase_2.

However, if you use Git rebase, you move your whole feature branch, starting it on the tip of the main branch so that all the new commits are now part of the whole. This action rewrites the project history by making new commits for each of the original branch's commits. So, this is how the new branch looks:

Git_Rebase_3.

Source

What Is Git Rebase: Git Rebase Usage
Why do people use Git rebase? For one overriding reason: maintaining a linear project history. Let's say for instance that you've been working on a feature branch off the main branch, but the latter has progressed. But you want to get the main branch's latest updates into your feature branch while keeping your branch's history clean, so it looks like you've been working off the updated, latest main branch.

You will benefit from an eventual clean merge of your feature branch back into the main branch, perpetuating a clean history. It's essential to have a clean history, especially when conducting Git operations, to locate and investigate a possible regression introduced into the branch.

To sum it briefly, when you conduct a Git rebase, you’re saying that you want your changes to be based on what other developers have already done.

Basics to Advanced - Learn It All!

Git Rebase Standard vs. Git Rebase Interactive
There are two different Git rebase modes, standard and interactive. A standard mode Git rebase automatically grabs the commits present in your current working branch and immediately applies them to the head of the passed branch.

On the other hand, Interactive mode lets you change different commits in the process instead of just scooping up everything and tossing it into the passed branch. If you use interactive mode, you can remove, split, or alter existing commits and clean up the history, and we've already touched on why clean histories are essential.

So how do you perform a Git rebase? 

How to Git Rebase
Here’s the syntax for launching a standard Git rebase:

git rebase <base>

And here’s the syntax for launching an interactive Git rebase:

git rebase --interactive <base>

This command opens an editor that lets you enter commands for each commit you want to rebase.

Later, we’ll explore a broader range of rebase commands. But before we do, we must discuss configuration.

---Git Hub Token { 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 } 03/04/2023

-----GIT TASKS:
---1.How to do SSH with Github repository
---2.Find out particular_user commits
---3.Find out febraury 25th commits(Particular Date)
---4.Find out how many commits happend in particular_file (e.g: file1.txt)
---5.In Particular commit How many changes i did?
---6.How many commits i did between particular date to particular date or last 3days.
---7.How to change the commit message.
---8.How to revert changes from staging area.
---9.How to revert the changes after git command.
---10.Assume there are 4 files which is having some similarity in the names and you want to track the files from working directory to local directory. Do....
---11. Difference between git pull and git fetch
---12.How to revert git merge
---13.How to delete a branch and how to revert deleted branch

##################################################################################################
About MAVEN:
---MAVEN is chiefly used for Java-based projects. This tool helps in building the code downloading dependencies. It simplifies the day to day work of Java Developers and helps them in the project.
---Maven is a Build Tool which is Developed by Apache Software Foundation and it is a Open Source Tool.
---Let me take a example, if you are having Java Project we need some third party dependencies. suppose if you are working with MySQLDatabase you need JAR Files{Java Archived Files}, If you are working with Testing(selenium) you need JAR Files and attach to Java Project.
---In Manual Process for those dependencies you will go to different websites and download the dependencies and attach to java project.
---Suppose if you want to improve the version of your project, you need to improve the version of dependencies as well. again you need to remove the existing prject and will again add new versions.
---To Overcome this issues MAVEN Tool comes into the picture.
---Whenever You creating Maven Project by default it creates pom.xml file.
---In pom.xml file it contains the Dependencies and plugins
---When you are having Dependencies on pom.xml it will automatically download third party applications for the project.
---Plugins another type of entires which is in the pom.xml, it is basically having configuration stuff like compiling the project, running the project and some other type of configurations which you specify inside the plugins.
---This Two Plugins and Dependencies Will control entire project.
---Maven also provides Project Structure
---It can generate Project related reports and Documentation.
---Maven will do the packaging of the project.
----How Maven Project Works Internally? Like how it downloads the dependencies from different websites and how it is attaching to the project.
---Maven is based on "Project Object MOdel" and focuses of simplication and standardization of the Building Process.
---In the pom.xml {Project Object Model}file we have <dependencies> tag will be there. Here we will specify the Dependencies for entire project.
---As soon as you are creating maven project, it will create maven local repository internally ".M2 directory". As soons as you are giving dependencies in the tag, It will download the JAR files of dependencies into local repository. Actually where it download the dependencies, it downloads from the Remote Maven Repository.
---Website Name: mvnrepository.com
---Remote Maven Repository conatins the all JAR Files, Whenever you are specifying the dependencies in the pom.xml file, it will check the remote maven repository whether the specified version of that dependencies are there or not.
---To get all the JAR Files we should have internet connection.
---Once all the Dependencies downloaded into the local repository which is in your local system in that case you don't need any internet connection.
---If you want to update version of the dependencies, you need to specify in the pom.xml.  
---What is the use of Plugins: Project Related Configurations
---Suppose if you want to compile a project, then we need to add maven compiler plugin.
---If you want to run the project, then we need to add surefile plugin.
---Compiler: Compile refers to the act of converting programs written in high level programming language, which is understandable and written by humans, into a low level binary language understood only by the computer.

@@@@_Pre-Requisites to install Maven in your Server:
--- Server Configuration t2.micro, AMI: Any
--- Java Application(Oracle Corporation)
    (sudo amazon-linux-extras install java-openjdk11) ---- (yum install java-11-openjdk)
--- Git (In ubuntu git is installed by default)
--- Install maven.
--- Open WebBrowser and Search for " https://downloads.apache.org/maven/maven-3/ "
--- Select Binaries Here
--- Copy the link here
--- Download the tar file and Un-tar file
--- Maven Repo is installed
--- yum install maven -y
--- mvn archetype:generate (It Downloads Maven Related Repositories, Documents, plugins will be downloaded over internet)
---What is JDK:- Java Development Kit
---What is JRE:- Java RunTime Environment
---Difference JDK & JRE: In JDK Will get Compiler by default, but not in JRE.
---JavaC - Java Compiler.
###################################################################################################################################
---MAVEN LIFE CYCLE::: (LifeCycle---Phases---Plugins---Goal)
--- (Build/Default, Clean, SiteLifecycle)
--- Build Life Cycle Consists of a sequence of build phases and each build phase consists of a sequence of goals.
---Each goal is resposnible for a particular task.
---When a phase is run all the goals related to that phase and its plugins are also compiled.
---Let us take example of Build Life Cycle. Here, different phases will be there they are validate, compile, test, packaging and install.
--- For Compile you need to give " compiler plugin " in POM.xml
--- For Test you need to give " SureFire Plugin " in POM.xml
--- For package you need to give JAR Plugin, War Plugin, Ear Plugin.
--- JAR Means JAVA ARCHIVED FILES
--- WAR Means WEB ARCHIVED FILES
--- EAR Means ENTERPRISE ARCHIVED FILES
--- Maven CleanLifeCycle (PreClean---Clean---PostClean)
--- Maven SiteLifeCycle (PreSite---Site---PostSite)

---> MAVEN COMMANDS:
* mvn compile
* mvn test
* mvn package
* mvn install
* mvn deploy
* mvn clean install --- It will do all the build phases at a single shot
* mvn clean install -D maven.test.skip=true --- It Skips the test

###################################################################################################################################
                                                         JENKINS
###################################################################################################################################
Jenkins:
========
--> Jenkins is developed on Java and it is a CI/CD (Continuous Integration/Continuous Delivery) tool.
--> Repeated tasks will be done by Jenkins.
--> Default port for Jenkins is 8080.

Advantages of Jenkins:
======================
--> It is an open source tool with great community support.
--> It is easy to install.
--> It has 1000+ plugins to ease your work. If a plugin does not exist, you can code it and share with the community.
--> It is built with Java and hence, it is portable to all the major platforms

Terminologies:
==============
Jenkins Home Directory(/var/lib/jenkins):
-----------------------------------------
    -->This is default home directory for our jenkins.
       Where my all jenkins configuration such as jobs, users, plugins information will be saved.

Build Directory(/var/lib/jenkins/jobs/<jobname>):
-------------------------------------------------
    --> Where build information will be saved (Always build information in master).

Workspace/Root Directory(/var/lib/jenkins/workspace/<jobname> if it is a master):
----------------------------------------------------------------------------------
    --> Where build steps will be triggered and SCM steps also executed here. This can be master or slave.
 
--> We can remove the old builds in jenkins console.
    ->Under Configure -> Discard Old Builds

###########################################################################################################################
 Jenkins job steps:
1) job name/description/where to run
2) where is ur code. (SCM)
3) when do u want to run this job (build trigger)
4) what build steps  to run (build steps)
> build an app
> sonar checks
> nexus upload
> deploy into dev
> notification
###############################################################################################################################

Jenkins supports different types of build jobs. Some of them are:
 
Freestyle Project
-----------------
Freestyle build jobs are general-purpose build jobs, which provides a maximum of flexibility. It can be combined in SCM with any build system and this can be even used for something other than software build. It is the central feature of Jenkins.
 
Maven Project
-------------
Build a maven project. Jenkins takes advantage of your POM files and drastically reduces the configuration
 
Pipeline
--------
Here we can easily identify which step is failing no need to check in console output. Everything will be in script and will get the individual stages of output which helps us to understand where it is failing. Pipeline will be in Groovy Script.
 
Multibranch Pipeline

It will have more functionalities when compare with pipeline such as auto creation/deletion of jobs.

################################################################################################################################

I am running my job in slave machine
 
Feature job >> daily builds will be happend when developer commited the code
 
When developer commited new code to repo my jenkins daily build will be triggered
 
git checkout into intermediate or dev server >> git should be installed on slave
 
application build >> maven is required on slave machine.
 
sonar checks >> sonar can be installed any where and sonar info should be added to jenkins master
 
nexus >> nexus  can be installed any where and nexus info should be added to master.
 
web deployment into tomcat >> tomcat can be installed on same machine or different machine and add the info to jenkins master.


Jenkins build Steps:
====================
git checkout
maven build
sonar checks
nexus upload
deploy war file to tomact server

#############################################################################################

How to add Slave:
--> Need to add shell /bin/bash for jenkins user in /etc/passwd.(master)
--> Login to master with jenkins user (su - jenkins).
--> In slave set the password for root user and enable the password authentication to "Yes" in /etc/ssh/sshd_config and restart the sshd service.
    If it is a normal user then we have to add it in sudo as well.
--> Do the ssh to slave from Master with root.
--> Install the java in slave (must and should java required in slave)
    (https://www.tecmint.com/install-apache-maven-on-centos-7/)
--> Finally add slave into jenkins from console.

#################################################################################################
sudo update-alternatives --config java
--> How to reset user password in jenkins
    -> Manage Jenkins -> manage users >> Click on user -> Go to Configure -> Password ( give new password and save)
 
--> If we want to give any previliges to any user.
    -> Manage Jenkins -> Configure Global Security -> Project based matrix -> Add user or Group -> Give Permissions -> save
--> If we want to stop/disable the jobs/project.
    -> Configure -> Disable the project
--> If admin forgot password
    i) Go to putty(server) and jenkins home directory
    ii) #cd /var/lib/jenkins
    iii) #vi config.xml
        <usesecurity>true</usesecurity> (By default it will be true)
        change from true to false and then restart the service
    iv) This time GUI will not ask the credentials to login, it will login directly
--> Once we login if we want to revert the changes then,
    i) manage jenkins -> configure global security -> enable the security -> jenkins own user database -> save
    ii) Go to manage users -> select the user and reset the password under configure
    iii) If we change password in GUI and enabled then in the config.xml file it will become true.
 
###############################################################################################################

Plugins:
========
--> We can integrate any tool to jenkins with plugins.
 
--> Installed plugins will be available under below,
    manage jenkins -> manage plugins -> installed
 
--> Available plugins will be under below,
    manage jenkins -> manage plugins -> available
 
--> If we want to install any plugin then go to available tab and search with plugin and click on install
    ex: maven
######################################################################################################
Maven:
======
--> It is a developer tool/build tool for java project.
--> As a DevOps admin we just need to install the maven.
--> The path for maven is: /etc/profile.d
                vi maven.sh
--> The below are the life cycles of maven(means commands), Developers will give the life cycles to us.
    validate, compile, package, test, verify, install, deploy
##########################################################################################################

Installation of Sonarqube and Nexus:
====================================
--> Sonarqube is to verify the code quality interms of bugs,vulnerabilities.
 
--> As a devops admin we just need to install, configure and integrate to jenkins.
 
--> Sonarqube will generate the reports.
 
--> Nexus is just to store the data with security.
 
Port Numbers:
=============
Tomcat    -- 8080
Nexus    -- 8081
Sonarqube -- 9000
Jenkins    -- 8080
#####################################################################################################

---> Send automated email to the dev team if build fails
a.	In Order to send and receive emails we need to configure email servers in jenkins.
Companies has their own email servers, we have to use those servers to trigger   
Emails, in our example we do not have our own server, so we are going to use
Gmails SMTP server.
b.Configure gmail SMTP server in jenkins
i.	jenkins → Manage Jenkins → Configure System
ii.	Under E-mail Notification
1.	SMTP server  → smtp.gmail.com
2.	Select Use SMTP Authentication
3.	Put your gmail id
4.	Put your gmail password
5.	Use SSL select
6.	SMTP port  465  
7.	Save the configuration

c.	Configure jenkins job to trigger email if build fails
i.	Open your job → Configure → Post Build Actions → Add post-build action → email notification
ii.	Under Recipients put teams mail id and save the configuration

=================================================================================================================================================================
                                                               TROUBLESHOOTING FAILED JOBS
====================================================================================================================================================================

---> Build Failed due to host key verifucationn is Failed while executing scp command
     scp -v -o StrictHostKeyChecking=no /path_artifacory privateIP:/path_webapps

################################################################################################################################
Nexus config:
 
=============
/usr/local/src/apache-maven/conf/settings.xml
## add below in </servers> componenet
    <server>
    <id>deployment</id>
    <username>admin</username>
    <password>admin123</password>
    </server>

/usr/local/src/apache-maven/bin/mvn package
 
/usr/local/src/apache-maven/bin/mvn sonar:sonar -Dv=${BUILD_NUMBER} deploy
 
<distributionManagement>
   <repository>
   <id>deployment</id>
   <name>Internal Releases</name>
   <url>http://3.8.82.180:8081/repository/maven-releases/</url>
   </repository>
   <snapshotRepository>
   <id>deployment</id>
   <name>Internal Snapshot Releases</name>
   <url>http://3.8.82.180:8081/repository/maven-snapshots/</url>
   </snapshotRepository>
</distributionManagement>


Pipeline syntax:
pipeline{
agent { ‘label labelname’ }
stages{
stage(scm){
steps{ 
}
                    }
stage(sonar){
steps{
         }
                       }
            }
              }


---> Pipeline example:
pipeline{
     agent { label 'slave' }
     stages{
       stage(scm) {
           steps{
checkout([$class: 'GitSCM', 
    branches: [[name: '*/feature']], 
    userRemoteConfigs: [[credentialsId: '4a1288aa-6391-4d75-9387-f37f8a43bd1d ', url: 'https://github.com/devopsamar/mavenrepo.git']]
])
}
       } 
       stage(package){
       steps{
          sh '/usr/local/src/apache-maven/bin/mvn package' 
       }    
       }
       stage(tomcat){
           steps{
          sh 'ssh 18.188.185.28 systemctlfff stop tomcat'
sh 'scp /root/workspace/My_First_Job/target/studentapp-2.5-SNAPSHOT.war 18.188.185.28:/var/lib/tomcat/webapps'
sh 'ssh 18.188.185.28 systemctl start tomcat'
       }
     }
     }
         }


#################################################################################################################################

*******Difference Between POLL SCM & BUILD PERIODICALLY************
"Poll SCM" polls the SCM periodically for checking if any changes/ new commits were made and shall build the project if any new commits were pushed since the last build, whereas the "build periodically"  shall build the project periodically irrespective to whether or not any changes were made.

*******Difference Between POLL SCM & GitHub WebHooks************
POLL SCM checks Source Code Management Tool periodically for if any changes/new commits were made and shall build the project if any any new commits were pushed since the last build. GitHub WebHooks will poll source code management automatically whenever the changes committed in SCM without concerning time and date. 

##################################################################################################################################

what is webhook? diff between webhook and scm

webhook: webhook is whenever we do any changes in the github repository then the job in jenkins where that github repository is used gets triggered automatically


login to github> go to your repository ex: sample-satwick/sample_application > go to settings > click on webhook> create webhook> 

Payload URL
http://65.0.105.99:8080/github-webhook/   		(this url is where jenkins is running and add github-webhook/ at the end)

Content type
application/json

save it
	
and in the jenkins job add this github repository and
in build triggers select "GitHub hook trigger for GITScm polling"  save the job 
now when u do any changes like add new file in github repository of sample_application then automatically in jenkins the job gets triggered.


diff between webhook and scm:
------------------------------

While polling and webhooks both accomplish the same task, webhooks are far more efficient.
When using polling, the frequency of your polls limits how up-to-date your event data is. For example, if your polling frequency is every 12 hours, 
the events returned by any poll could have happened any time in the past 12 hours. This means that any time an event occurs in the endpoint, your app 
will be out-of-date until the next poll.

With webhooks, this problem is eliminated. Since events are posted immediately to your monitored URL, your apps will automatically update themselves with 
the new data almost instantly.


#####################################################################################################################################
                                                              TOMCAT
#####################################################################################################################################
---> We have two different types of websites:
1.Static Website: This website contains same type of info which can be visible to all the users, It Responds fastly comparing to Dynamic type application website and maintaining this static website is very easy comparing to Dynamic website.

2.Dynamic Website: This Website changes dynamically user to user and it responds little bit late comparing to Static Website.

Tomcat: Tomcat is application server. It is java based application. It provides a platform to the java application.

Difference Between Web server and Application server

          80                                                                            8080
        HTTPD                                                                          TOMCAT
 /etc/httpd/conf/httpd.confg                                                     /opt/apache-tomcat-xx/conf
    /var/www/html                                                               /opt/apache-tomcat-xx/webapps
     .html/.php                                                                        .java

PREREQUISITE TO INSTALL TOMCAT:
1.Java
2.Tomcat

----Download Tomcat Tar File
----Untar the downloaded File
----https://www.digitalocean.com/community/tutorials/how-to-install-apache-tomcat-7-on-centos-7-via-yum   

##########################################Troubleshooting Tomcat Issues:#################################################

---> Before accessing the tomcat application in web browser make sure that Application is started or not in the terminal.
* systemctl status tomcat

---> Make sure that you should open port number 8080 in security Groups

---> 403 Access Denied ----> You are not authorized to view this page. 
If you are getting this error, you need to Make some changes in "Conf/tomcat-users.xml", add the below line on that file at the bottom of the file:
* vi /opt/tomcat/conf/tomcat-users.xml
* <user username="tomcat" password="password" roles="manager-gui"/>

---> Even After creating role in Conf/tomcat-users.xml in this file and you are facing the same issue means then do,
* vi /opt/tomcat/webapps/manager/META-INF/context.xml

In this file find the below lines

<Valve className="org.apache.catalina.valves.RemoteAddrValve"
         allow="127\.\d+\.\d+\.\d+|::1|0:0:0:0:0:0:0:1" />
  <Manager sessionAttributeValueClassNameFilter="java\.lang\.(?:Boolean|Integer|Long|Number|String)|org\.apache\.catalina\.filters\.CsrfPreventionFilter\$LruCache(?:\$1)?|java\.util\.(?:Linked)?HashMap"/>


#comment those lines with "<!--" starting of the line and give  "-->" at the end of line.

---> After Commenting those lines, it looks like below:

<!--  <Valve className="org.apache.catalina.valves.RemoteAddrValve"
         allow="127\.\d+\.\d+\.\d+|::1|0:0:0:0:0:0:0:1" />
  <Manager sessionAttributeValueClassNameFilter="java\.lang\.(?:Boolean|Integer|Long|Number|String)|org\.apache\.catalina\.filters\.CsrfPreventionFilter\$LruCache(?:\$1)?|java\.util\.(?:Linked)?HashMap"/>  --> 

Now, restart tomcat application and check with webbrowser by accessing it.
* systemctl restart tomcat.

###################################################################################################################################
                                                            SONARQUBE
###################################################################################################################################
IN DevOps Developing the code is essential one
Anyone needs to check the code written by Developer
Is it bug free?
Is it secure?
No Duplications? (We can use same type of codes at different places, checking whether they have used it or not)
Tested Properly?
Complex Code? (If developers writes complex code others may feel typical to check the bugs and errors in code)
Easy to Integrate with others Code?
----------------------------------------

* For this Purpose we need to check the code. Who Will check the code?
-> The person who is having coding knoweldge otherthan original developer will check the code. This is called Peer Review.
-> This is manual process and Time consuming process and productivity also decreases.
-> In this case we should have automation tool to run these activites.

#STATIC_CODE_ANALYSIS: This will check what are the problems that are affecting to the code and it will give Review Reports and which allow the developer to check what are the problems in the code and also helps the developer where to develop the code.

Static_Code_Analysis Tools: SonarQube, Coverity, raxis, VERACODE, CodeScene.

### SONARQUBE: This tool not only used for Static Code Analysis and also it is used for Quality Analysis("Quality Management Tool").
- Code Analysis
- Test Reports
- Code Coverage
It shows Reports in Graphical User Interface(GUI)

Components Of SonarQube
--> Sonarqube Server
* Rules: Rules are nothing but conditions, To check bugs, Security Vulnerabilites, Code Complexity are checked. To test all these things we give set of rules. By default you will get rules when you install Sonarqube into the server.
* Database: Rules are Implied on particular code, it will generate the report. That reports are stored in Database.
* WebInterface: It is used to view the data which is stored in Database
* Elastic Search: It is used to search for a Particular report stored in database.

---> SonarScanner
* It is agent of sonarqube
* It is used to run on the source code on the developer system.
* SonarQube supports 27 Programming languages.
* It generates Reports and send it to SonarQube Server.


Sonarqube available in 2 editions
Commercial:
ABAP, C-Family (C, C++, and Objective-C), COBOL, PL/SQL, Visual Basic,
Natural, VB.Net, RPG, Swift..
Open Source:
Java, Java Script, C#, Web(HTML, JSP, JSF, ..) XML, Python, Groovy,
PHP, Puppet, Lua, Groovy, FxCop, Flex, Erlang ...
____________________________________________________________________________________________________________________________________
SonarQube installation in Linux:
Hardware Requirements for SonarQube
---------------------------------------------------------------------------------------------------------------
* The SonarQube server requires at least 2GB of RAM to run efficiently.

---> https://www.fosstechnix.com/how-to-install-sonarqube-on-ubuntu-22-04-lts/
---> SONARTOKEN: f938d86c2cdba8fe73953ca8a8ab739e0642f03c
----> GMail Generated Token: rbghuiaeviasqpxk

* Login as a root user.

Note: MySQL Support for SonarQube is depricated. Increase the vm.max_map_count kernal ,file discriptor and ulimit for current session at runtime.

Give the Below Commands

* sysctl -w vm.max_map_count=524288
* sysctl -w fs.file-max=131072
* ulimit -n 131072
* ulimit -u 8192

To Increase the vm.max_map_count kernal ,file discriptor and ulimit permanently . Open the below config file and Insert the below value as shown below,

sudo nano /etc/security/limits.conf

sonarqube   -   nofile   65536
sonarqube   -   nproc    4096

Before installing, Lets update and upgrade System Packages
* sudo apt-get update
* sudo apt-get upgrade

(Upgrades go from version to version, for example, version 2019 to version 2020. Updates are small changes within the same version, for example, version 2019.1. 0 to 2019.2.)

Install wget and unzip package
* sudo apt-get install wget unzip -y

Step #1: Install OpenJDK
sudo apt-get install openjdk-11-jdk -y
sudo apt-get install openjdk-11-jre -y

SET Default JDK
To set default JDK or switch to OpenJDK enter below command,
* sudo update-alternatives --config java

Check JAVA Version:
* java -version

Step #2: Install and Setup PostgreSQL 10 Database For SonarQube
* sudo sh -c 'echo "deb http://apt.postgresql.org/pub/repos/apt/ `lsb_release -cs`-pgdg main" >> /etc/apt/sources.list.d/pgdg.list'

*  wget -q https://www.postgresql.org/media/keys/ACCC4CF8.asc -O - | sudo apt-key add -

Install the PostgreSQL database Server by using following command,
* sudo apt-get -y install postgresql postgresql-contrib

Start PostgreSQL Database server
* sudo systemctl start postgresql

Enable it to start automatically at boot time.
* sudo systemctl enable postgresql

Change the password for the default PostgreSQL user.
* sudo passwd postgres

Switch to the postgres user.
* su - postgres

Create a new user by typing:
* createuser sonar

Switch to the PostgreSQL shell.
* psql

Set a password for the newly created user for SonarQube database.
* ALTER USER sonar WITH ENCRYPTED password 'sonar';

Create a new database for PostgreSQL database by running:
* CREATE DATABASE sonarqube OWNER sonar;

grant all privileges to sonar user on sonarqube Database.
* grant all privileges on DATABASE sonarqube to sonar;

Exit from the psql shell:
* \q

Switch back to the sudo user by running the exit command.
* exit

Step #3: How to Install SonarQube on Ubuntu 22.04 LTS

Download sonaqube installer files archive To download latest version of visit SonarQube

*  cd /tmp
* sudo wget https://binaries.sonarsource.com/Distribution/sonarqube/sonarqube-8.9.9.56886.zip

Unzip the archeve setup to /opt directory
* sudo unzip sonarqube-8.9.9.56886.zip -d /opt

Move extracted setup to /opt/sonarqube directory
* sudo mv /opt/sonarqube-8.9.9.56886 /opt/sonarqube

Step #4:Configure SonarQube on Ubuntu 22.04 LTS
1. Create Group and User:
Create a group as sonar
* sudo groupadd sonar

Now add the user with directory access
* sudo useradd -c "user to run SonarQube" -d /opt/sonarqube -g sonar sonar 
* sudo chown sonar:sonar /opt/sonarqube -R

Open the SonarQube configuration file using your favorite text editor
* sudo nano /opt/sonarqube/conf/sonar.properties

Uncomment and Type the PostgreSQL Database username and password which we have created in above steps and add the postgres connection string.

#sonar.jdbc.username=sonar
#sonar.jdbc.password=sonar
sonar.jdbc.url=jdbc:postgresql://localhost:5432/sonarqube
Uncomment and Type the PostgreSQL Database username and password which we have created in above steps and add the postgres connection string.

Edit the sonar script file and set RUN_AS_USER
* sudo nano /opt/sonarqube/bin/linux-x86-64/sonar.sh

2. Start SonarQube:
Now to start SonarQube we need to do following: Switch to sonar user
* sudo su sonar
Move to the script directory
* cd /opt/sonarqube/bin/linux-x86-64/
Run the script to start SonarQube'
* ./sonar.sh start

3. Check SonarQube Running Status:
To check if sonaqube is running enter below command
* ./sonar.sh status


How to view sonar console?
* before accessing the sonar tool, we should add 9000 port to instance security group
* copy your instance Ip address and paste it in ipaddress
ex: 139.59.34.76:9000
* it will ask you for username and password
 userid admin
 password admin

* it will redirect you to password change page. update your password

TroubleShooting Errors:
--------------------
sonar service is not starting?
a)make sure you need to change the ownership and group to /opt/sonarqube/ directory for
sonar user.
b)make sure you are trying to start sonar service with sonar user.
c)check java is installed or not using java -version command.
d)delete /temp file under sonarqube directory and try to start sonar
Unable to access SonarQube server URL in browser?
a)make sure port 9000 is opened in security groups - AWS ec2 instance.
de135dcbfcf7c37ddb3b1b6f718d323f4c5d0fdc
___________________________________________________________________________________________________________________________________
Create SonarQube server as a sonar service
--------------------------------------------------------
Running SonarQube as a Service on Linux with SystemD

On a Unix system using SystemD, you can install SonarQube as a service. You
cannot run SonarQube as root in unix systems. Ideally, you will have created a new
account dedicated to the purpose of running SonarQube. Let's suppose:
• The user used to start the service is sonarqube
• The group used to start the service is sonarqube
• The Java Virtual Machine is installed in /opt/java/
• SonarQube has been unzipped into /opt/sonarqube/
• create a file /etc/systemd/system/sonar.service using vi editor
• cmd: vi /etc/systemd/system/sonar.service

paste the below code into sonar.service file
[Unit]
Description=SonarQube service
After=syslog.target network.target
[Service]
Type=forking
ExecStart=/opt/sonarqube/bin/linux-x86-64/sonar.sh start
ExecStop=/opt/sonarqube/bin/linux-x86-64/sonar.sh stop
LimitNOFILE=65536
LimitNPROC=4096
User=sonar
Group=sonar
Restart=on-failure
[Install]
WantedBy=multi-user.target

• Enable the sonar service
sudo systemctl enable sonar
• Start the sonar service
sudo systemctl start sonar
• Check the status of the sonar service
sudo systemctl status sonar

Note: For ubuntu Document please follow the below link
https://www.how2shout.com/linux/install-sonarqube-on-ubuntu-20-04-18-04-server/
____________________________________________________________________________________________________________________________________
NEXUS INSTALLATION PART
========================
---> Pre-Requisites: t2.small instance.

Install Nexus on Ubuntu:
apt-get update
apt-get install unzip
apt install openjdk-8-jre-headless -y
cd /opt
wget http://download.sonatype.com/nexus/3/nexus-3.22.0-02-unix.tar.gz
chmod 777 nexus-latest-bundle.zip (#Use the correct version as per downloaded.)
tar -zxvf nexus-3.22.0-02-unix.tar.gz
sudo adduser nexus
chown -R nexus:nexus nexus-3.22.0-02/
chown -R nexus:nexus sonatype-work/
rm -rf nexus-latest-bundle.zip
cd nexus-3.22.0-02/bin
RUN_AS_USER=nexus ./nexus start
http://65.2.188.153:8081/nexus/#welcome

Open settings.xml file will be there, if you don't know the path give command

$ find / -name settings.xml

It will prints you the path and open it in vi editor, there you will find the server open tag <servers> below that one give the following data

<server>
      <id>deployment</id>
      <username>admin</username>
      <password>admin</password>
</server>
______________________________________________________________________________________________________________________________________

####################################################################################################################################################################
                                                                     DOCKER
####################################################################################################################################################################

MONOLITHIC: We can take example on this like different types of services available in a single application is called Monolithic. E.g: Phonepe is the example for that we have different types of services available in phonepe like banking, movie tickets, Train Tickets, Muncipal Tax payments etc., if you want to update the feature of one service you need stop all services for update. Here, Entire application is deployed into one server.

MICRO SERVICES: In Micro Services Applications are stored in Different servers and if you want to update the feature of one service you don't need to stop entire application. you can update the server where service is stored. The cost of maintaining the servers is hign comparatively to MONOLITHIC.

Here, Docker comes into the picture. We can overcome this issue with the help of Docker.

DOCKER: Docker is the "containerzation Tool", Conatiner is logical vertual machine which is created within in the Single Server. Each container is isolated from other container. The main purpose of using docker is to manage the cost and time. It is advanced than vertualization.

---> It is an Open-source Centralized Platform designed to create, deploy, and run applications.
---> Docker is written on go language.
---> Docker uses container on host OS to run applications.
---> We can install docker on any OS but docker engine natively runs on linux distribution.
---> Docker performs OS level vertualization also known as Containerization.
---> It was intially release in march 2013 and developed by solomonhykes and sebastian pahl.


What is difference between Virtualization and Containerization?

* Virtualization advantages and Dis-advantages:

Adv:
a)Multiple OS in Same Machine
b)Easy Maintenance and Recovery
c)Lower total cost of Ownership

Dis-adv:
a)Multiple Vm's lead to unstable performance
b)Hypervisors are not as efficient as Host OS
c)Long Boot-Up-Process (Approx. 1 minute)

---> Containerization: Containerization is just virtualization at the Operating system level:

Advantages over Virutalization:
a)Containerization on same OS kernel are lighter and smaller
b)Better resource utilization compared to VM's
c)Short boot up process (1/20th of a second)

---> Docker Daemon: Daemon run on host machine. Daemons create and manage Docker objects: Images, Containers, Networks, Volumes, Data, etc. The user does not directly interact with the Daemon, but instead through the Docker client.

---> Docker Client: Primary user interface to Docker. It accepts commands from the user and communicates back and forth with a Docker daemon.

---> Docker Images: Images are used to create Docker containers. Docker provides a simple way to build new images or update existing images. Docker images are the build component of Docker.
 
---> Docker Registries: Registries store images. These are public or private stores from which you upload/download images. This can be done on Docker Hub, which is Docker's version of Git Hub. Docker registries are the distribution component of Docker.

---> Docker Containers: Containers are created from Docker images. They hold everything that is needed for an application to run. Each container is an isolated and secure application platform. Docker containers are the run component of Docker.

---> How to create Dockerhub Account?
Search www.Dockerhub.io in Google
Select signup option
Enter Your Name, Mail ID, Password and signup.

---> How to install Docker in ec2-Instance???
Step 1 — Installing Docker

First, update your existing list of packages
* sudo apt update

Next, install a few prerequisite packages which let apt use packages over HTTPS
* sudo apt install apt-transport-https ca-certificates curl software-properties-common

Then add the GPG key for the official Docker repository to your system
* curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -

Add the Docker repository to APT sources
* sudo add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu focal stable"
This will also update our package database with the Docker packages from the newly added repo

Make sure you are about to install from the Docker repo instead of the default Ubuntu repo
* apt-cache policy docker-ce

Finally, install Docker
* sudo apt install docker-ce

Docker should now be installed, the daemon started, and the process enabled to start on boot. Check that it’s running
* sudo systemctl status docker

--->Docker Compose:Compose is used to build and running multi-container docker application during development

Here, 1. we will create docker yaml file   e.g: docker-compose.yml
      2. we will write come instructions in that yaml file like while we are creating Dockerfile we are giving commands RUN, ADD, ENTRYPOINT---
          in the same way we will give some instructions in yaml file.
      3. Docker-Client will understand those instructions and Translate those instructions into the commands and send it to the docker server.
      4. It creates containers.
      5. One added advantage to docker compose is that it creates the network automatically.

SYNTAX: 

  version: ''

  services: 
     service-name:
     image:
     expose:
        - ''
     
Docker compose will run where the docker-compose.yml file is saved on directory.

docker-compose up
docker-compose down
docker-compose ps --- it will show all the containers currently running.
        



Docker COMMANDS:
* If you want to check images in your Local Machine.
 $ docker images

* If you want to pull Image from Docker Hub
 $ docker pull Image_Name

* If you want to create container
 $ docker run -dit image_name

* If You want to check running containers
 $ docker ps

* If you want to start the Container
 $ docker start container_id

* If you want to stop the container
 $ docker stop  container_id

* If you want to check the total number of containers Including Running & Exited
 $ docker ps -a

* If you want to delete the conatiners
 $ docker rm container_ids

* If you want to logged into the container
 $ docker exec -it container_id /bin/bash

---> If you want to install any application in your container you need to update your system
 * if it is ubuntu
  $ apt update
 * If it is Centos/amazon-linux/Redhat
  $ yum update

---> Types of Docker Network:
1. Bridge Network: 
2. Host Network: If container used dockerhost network it is called dockerhost network.
3. Null Network:  

---> To check Networks of Docker
$ docker network ls 

---> How the end users will connect to the web application which is deployed in container of docker host??
$ docker run -dit -p 80:80 imagename
                     |
                   Hostport

---> If you want full information about the container like about volumes, network, image.
$ docker inspect ContainerId

---> If you want to see more details on the network associated with Docker, you can use the Docker network inspect command.
$ docker network inspect networkname(Bridge)

---> One can create a network in Docker before launching containers. This can be done with the following command 
$ docker network create –-driver drivername name

e.g: $ sudo docker network create –-driver bridge newnet 

---> How to delete Network?
$ docker network rm newnet 

---> Types of Volumes: 
1. Anonymous Volume
2. Named Volume

---> How to create volume and attach it to container?
$ docker run -dit -v vol1:/usr/local/apache2/htdocs/ httpd

---> The path of volume in dockerhost is?
$ cd /var/lib/docker/volumes/vol1/_data

---> How to check volume size 
$ df -h /var/lib/docker/volumes/vol1/_data

---> How to delete Image
$ docker image rm image_name

--->Error response from daemon: conflict: unable to remove repository reference "ubuntu" (must force) - container 7a7b51c5b364 is using its referenced image 08d22c0ceb15
 If You got above error then you need to delete the container first and then you can delete image.
 $ docker rm containerId
 $ docker image rm imagename

--->How to create our own image and push it into the hub

Firstly, Create your own image with desired configuration

$ docker commit conatinerid imagename(gowraajay/myimg)

$ docker tag gowraajay/myimg gowraajay/dockerhub:myubuntu

$ docker login -u username
  password:

$ docker push tagname(gowraajay/dockerhub:myubuntu) 

Output:
=======
The push refers to repository [docker.io/gowraajay/dockerhub]
824dc9d1fb5c: Pushed
b93c1bd012ab: Pushed
myubuntu: digest: sha256:383753340f83e8b363ef7791f727e0c711bcd59c1b8801a0d16d1177c1323ee2 size: 742

---> Dockerfile: 
* It is basically a text file which contains some set of instructions.
* Automation of docker image creation.
* Always D is capital letter on Dockerfile
* And start the components also be capital letter.

---> Docker File Components:

* FROM: for base image this command must be on top of the file.
* RUN: to execute the command, it will create a layer in file.
* COPY: copy files from local system
* ADD: it can download files from internet and also we can extract file at dockerimage side.
* EXPOSE: to expose ports such as 8080 for tomcat and port 80 for nginx etc.
* WORKDIR: to set working directory for the container.
* ENV: environment variables.

 $ Docker build . -t gowraajay/httpd

$ docker run -it --name container_name img_name
========================================================================================================================================================================
                                                                      KUBERNETES(K8s)
========================================================================================================================================================================== 
---> Before Discussing about kubernetes we discuss about the Dis-Advantages of Docker. 
1. Suppose if Docker Container is down due to some reasons, Docker is not going to create new container automatically. It leads to application down time.
2. Docker Host doesn't create Replicas.
3. If you want to upgrade the containerized applications it required Application DownTime.
4. If Docker Host is down, Containers is also down.

---> Kuberenetes is an Open Source Orchaestrtation tool. It Orchaestrate the containers. It is first developed by Google and used in their project Borgi.
---> kuberenetes is having different functionalities like self healing, autoscaling, load balancers, Monitoring.
---> kuberenetes will run on top of containers.

Kubernetes Definition:
---------------------
It is a container orchestration tool able to simplify the management of containers and, simultaneously, to make it more efficient. 
The main aim of Kubernetes, as the other orchestration systems, is to simplify the work of technical teams, by automating many processes of applications and services deployment that before were carried out manually. 
In particular, now we’ll show you Kubernetes features that improve IT field’s work and the benefits for companies who decide to use it.

---> A Kubernetes cluster consists of a set of worker machines, called nodes, that run containerized applications. Every cluster has at least one worker node.
---> The worker node(s) host the Pods that are the components of the application workload. 
---> The control plane manages the worker nodes and the Pods in the cluster. In production environments, the control plane usually runs across multiple computers and 
     a cluster usually runs multiple nodes, providing fault-tolerance and high availability.

---> Kuberenetes Components:
 1.Master >>> etcd, Controller Manager, Scheduler
 2.Worker Nodes >>> Pods, kubelet

* The combination of Master and Worker Nodes is called Cluster.

---> CRI: The container runtime is the software that is responsible for running containers.
Kubernetes supports container runtimes such as containerd, CRI-O, and any other implementation of the Kubernetes CRI (Container Runtime Interface).

---> Kube Proxy: kube-proxy is a network proxy that runs on each node in your cluster, implementing part of the Kubernetes Service concept.

kube-proxy maintains network rules on nodes. These network rules allow network communication to your Pods from network sessions inside or outside of your cluster.
kube-proxy uses the operating system packet filtering layer if there is one and it's available. Otherwise, kube-proxy forwards the traffic itself.



Methods in Kuberenetes:
1.kubeadm - On premises servers
2.kops - Kuberenetes Operations
3.eks - AWS
4.aks - EKS
5.gks - GKS
6.Minikube

---> Kubernetes Alternative Tools: 
1. AWS Fargate
2. Azure Container Instances
3. Google Cloud Run
4. Google Kubernetes Engine (GKE)
5. Amazon Elastic Kubernetes Service (EKS)
6. Openshift Container Platform
7. Rancher
8. Docker Swarm

---> Kubernetes Installation on Amazon Linux Servers:
Pre-Requisites For Installation of k8s
Master - 4GB Ram
Nodes - 2vCPUs

---> Installing kubelet and kubeadm on all machines:
____________________________________________________
We will install the following packages on all the machines:

---> docker: the container runtime, which Kubernetes depends on. v1.12 is recommended, but v1.10 and v1.11 are known to work as well. v1.13 and 17.03+ have not yet been tested and verified by the Kubernetes node team.
---> kubelet: the most core component of Kubernetes. It runs on all of the machines in our cluster and does things like starting pods and containers.
---> kubectl: the command to control the cluster once it's running. We will only need this on the master, but it can be useful to have on the other nodes as well.
---> kubeadm: the command to bootstrap the cluster(the process of creating a new Kubernetes cluster from scratch and getting it up and running).

---> For each host, run the following machine:
----------------------------------------------
#1) Upgrade your Ubuntu servers
Provision the servers to be used in the deployment of Kubernetes on Ubuntu 22.04. The setup process will vary depending on the virtualization or cloud environment you’re using.

Once the servers are ready, update them.

 $ sudo apt update
 $ sudo apt -y full-upgrade
[ -f /var/run/reboot-required ] && sudo reboot -f

#2) Install kubelet, kubeadm and kubectl
Once the servers are rebooted, add Kubernetes repository for Ubuntu 22.04 to all the servers.

 $ sudo apt install curl apt-transport-https -y
 $ curl -fsSL  https://packages.cloud.google.com/apt/doc/apt-key.gpg|sudo gpg --dearmor -o /etc/apt/trusted.gpg.d/k8s.gpg
 $ curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
 $ echo "deb https://apt.kubernetes.io/ kubernetes-xenial main" | sudo tee /etc/apt/sources.list.d/kubernetes.list

Then install required packages.

 $ sudo apt update
 $ sudo apt install wget curl vim git kubelet kubeadm kubectl -y
 $ sudo apt-mark hold kubelet kubeadm kubectl

Confirm installation by checking the version of kubectl.

 $ kubectl version --client && kubeadm version

#3) Disable Swap Space
Disable all swaps from /proc/swaps.
 $ sudo swapoff -a 

Check if swap has been disabled by running the free command.

 $ free -h

Now disable Linux swap space permanently in /etc/fstab. Search for a swap line and add # (hashtag) sign in front of the line.

 $ sudo vim /etc/fstab

Confirm setting is correct
 $ sudo mount -a
 $ free -h

Enable kernel modules and configure sysctl.
# Enable kernel modules
 $ sudo modprobe overlay
 $ sudo modprobe br_netfilter

# Add some settings to sysctl
 $ sudo tee /etc/sysctl.d/kubernetes.conf<<EOF
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
EOF

# Reload sysctl
 $ sudo sysctl --system

#4) Install Container runtime (Master and Worker nodes)
To run containers in Pods, Kubernetes uses a container runtime. Supported container runtimes are:
 Docker
CRI-O
Containerd

1) Docker runtime
If your container runtime of choice is Docker CE, follow steps provided below to configure it.
# Add repo and Install packages
 $ sudo apt update
 $ sudo apt install -y curl gnupg2 software-properties-common apt-transport-https ca-certificates
 $ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -
 $ sudo add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable"
 $ sudo apt update
 $ sudo apt install -y containerd.io docker-ce docker-ce-cli

# Create required directories
 $ sudo mkdir -p /etc/systemd/system/docker.service.d

# Create daemon json config file
sudo tee /etc/docker/daemon.json <<EOF
{
  "exec-opts": ["native.cgroupdriver=systemd"],
"log-driver": "json-file",
  "log-opts": {
    "max-size": "100m"
  },
  "storage-driver": "overlay2"
}
EOF

# Start and enable Services
 $ sudo systemctl daemon-reload 
 $ sudo systemctl restart docker
 $ sudo systemctl enable docker

# Configure persistent loading of modules
 $ sudo tee /etc/modules-load.d/k8s.conf <<EOF
overlay
br_netfilter
EOF

# Ensure you load modules
 $ sudo modprobe overlay
 $ sudo modprobe br_netfilter

# Set up required sysctl params
 $ sudo tee /etc/sysctl.d/kubernetes.conf<<EOF
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
EOF

2) Installing CRI-O runtime
For CRI-O, use the commands shared below:

# Configure persistent loading of modules
sudo tee /etc/modules-load.d/k8s.conf <<EOF
overlay
br_netfilter
EOF

# Ensure you load modules
sudo modprobe overlay
sudo modprobe br_netfilter

# Set up required sysctl params
sudo tee /etc/sysctl.d/kubernetes.conf<<EOF
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
EOF

# Reload sysctl
 $ sudo sysctl --system

# Add Cri-o repo
 $ sudo -i
OS="xUbuntu_22.04"
VERSION=1.26
 $ echo "deb https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/$OS/ /" > /etc/apt/sources.list.d/devel:kubic:libcontainers:stable.list
 $ echo "deb http://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable:/cri-o:/$VERSION/$OS/ /" > /etc/apt/sources.list.d/devel:kubic:libcontainers:stable:cri-o:$VERSION.list
 $ curl -L https://download.opensuse.org/repositories/devel:kubic:libcontainers:stable:cri-o:$VERSION/$OS/Release.key | apt-key add -
 $ curl -L https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/$OS/Release.key | apt-key add -

# Install CRI-O
 $ sudo apt update
 $ sudo apt install cri-o cri-o-runc

# Start and enable Service
 $ sudo systemctl daemon-reload
 $ sudo systemctl restart crio
 $ sudo systemctl enable crio
 $ systemctl status crio

3) Installing Containerd
You can also use containerd instead of Docker, this is preferred method for Docker Engine users
# Configure persistent loading of modules
 $ sudo tee /etc/modules-load.d/k8s.conf <<EOF
overlay
br_netfilter
EOF

# Load at runtime
 $ sudo modprobe overlay
 $ sudo modprobe br_netfilter

# Ensure sysctl params are set
 $ sudo tee /etc/sysctl.d/kubernetes.conf<<EOF
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
EOF

# Reload configs
 $ sudo sysctl --system

# Install required packages
 $ sudo apt install -y curl gnupg2 software-properties-common apt-transport-https ca-certificates

# Add Docker repo
 $ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -
 $ sudo add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable"

# Install containerd
 $ sudo apt update
 $ sudo apt install -y containerd.io

# Configure containerd and start service
 $ sudo su -
 $ mkdir -p /etc/containerd
 $ containerd config default>/etc/containerd/config.toml

# restart containerd
 $ sudo systemctl restart containerd
 $ sudo systemctl enable containerd
 $ systemctl status containerd

To use the systemd cgroup driver, set plugins.cri.systemd_cgroup = true in /etc/containerd/config.toml. When using kubeadm, manually configure the cgroup driver for kubelet


#5) Initialize control plane (run on first master node)
Login to the server to be used as master and make sure that the br_netfilter module is loaded:

$ lsmod | grep br_netfilter

$ sudo systemctl enable kubelet

We now want to initialize the machine that will run the control plane components which includes etcd (the cluster database) and the API Server.

Pull container images:

 $ sudo kubeadm config images pull

# CRI-O
 $ sudo kubeadm config images pull --cri-socket /var/run/crio/crio.sock

# Containerd
 $ sudo kubeadm config images pull --cri-socket /run/containerd/containerd.sock

# Docker
 $ sudo kubeadm config images pull --cri-socket /run/cri-dockerd.sock 


These are the basic kubeadm init options that are used to bootstrap cluster.
https://computingforgeeks.com/install-kubernetes-cluster-ubuntu-jammy/


kubeadm join 172.31.15.31:6443 --token m5oc2g.2vf0bptufrieunfy \
        --discovery-token-ca-cert-hash sha256:1f3028dbd0c9d926f25b2ba3b7113f7232c45618f4d59f0937b40d3b58eebfa3


---> You should get above command to join the Worker Nodes into the Cluster.

---> open the port 6443 on master security group and copy the token from master and run it on all the worker nodes

---> Run on all worker nodes: ( you should get this token from above command)
kubeadm join 172.31.15.31:6443 --token m5oc2g.2vf0bptufrieunfy \
        --discovery-token-ca-cert-hash sha256:1f3028dbd0c9d926f25b2ba3b7113f7232c45618f4d59f0937b40d3b58eebfa3

---> run master node:

mkdir $HOME/.kube
cp /etc/kubernetes/admin.conf $HOME/.kube/config

kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml

If You Run The Above command you should get Below Output
                 |
                 |
namespace/kube-flannel created
clusterrole.rbac.authorization.k8s.io/flannel created
clusterrolebinding.rbac.authorization.k8s.io/flannel created
serviceaccount/flannel created
configmap/kube-flannel-cfg created
daemonset.apps/kube-flannel-ds created


root@ip-172-31-15-31:~# kubectl get nodes
 
NAME        STATUS   ROLES           AGE     VERSION
k8smaster   Ready    control-plane   9m59s   v1.26.3
k8snode1    Ready    <none>          2m31s   v1.26.3
k8snode2    Ready    <none>          2m35s   v1.26.3


---> Control plane:
==================
Protocol	Direction	Port Range	Purpose                     	Used By
==============================================================================================
TCP	Inbound	                 6443           Kubernetes API server	          All
TCP	Inbound	              2739-2380         etcd server client API	  kube-apiserver, etcd
TCP	Inbound		        10250           Kubelet API	           Self, Control plane
TCP	Inbound		        10259           kube-scheduler	                  Self
TCP	Inbound		        10257           kube-controller-manager	          Self

Although etcd ports are included in control plane section, you can also host your own etcd cluster externally or on custom ports.

---> Worker node(s):
====================
Protocol	Direction	Port Range	Purpose	              Used By
===================================================================================
TCP	Inbound	                   10250	Kubelet API	    Self, Control plane
TCP	Inbound	                30000-32767	NodePort Services†	   All

What is the Meaning of PODS?
pods are group of whales...!

---> KOPS Method Installation:
________________________________
Create ec2 instance >> attach the role (ec2full, s3full, IAM, Route53)

1. aws --version
2. curl -LO https://github.com/kubernetes/kops/releases/download/$(curl -s https://api.github.com/repos/kubernetes/kops/releases/latest | grep tag_name | cut -d '"' -f 4)/kops-linux-amd64

3. set the execution permission
  $ sudo chmod +x kops-linux-amd64

4. Move the kops to /usr/local/bin directory
  $ sudo mv kops-linux-amd64 /usr/local/bin/kops 

5. Configure AWS CLI with your Access Key ID,  Secret Access  key and region
  $ aws configure

6. Create the S3 bucket to store Kubernetes cluster states
  $ aws s3 mb s3://clusters.dev.ajay.com

7. Export kops state
  $ export KOPS_STATE_STORE=s3://clusters.dev.ajay.com

* $ vi /etc/profile   [add the above command export KOPS_STATE_STORE=s3://clusters.dev.ajay.com
 at the bottom of the file and save it..! ]

8. $ curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl

9. $ chmod +x ./kubectl

10. $ sudo mv ./kubectl /usr/local/bin/kubectl

11. $ ssh-keygen
cp -pr /usr/local/bin/kops /usr/local/sbin      
cp -pr /usr/local/bin/kubectl /usr/local/sbin

12. Install docker
  $ yum install docker

13. /usr/local/bin/kops create cluster --zones=ap-south-1b useast1.dev.ajay.com --dns-zone=ajay.com --dns private

Suggestions:
 * list clusters with: kops get cluster
 * edit this cluster with: kops edit cluster useast1.dev.ajay.com
 * edit your node instance group: kops edit ig --name=useast1.dev.ajay.com nodes-ap-south-1b
 * edit your control-plane instance group: kops edit ig --name=useast1.dev.ajay.com control-plane-ap-south-1b

Finally configure your cluster with: kops update cluster --name useast1.dev.ajay.com --yes --admin


14. /usr/local/bin/kops update cluster --name useast1.dev.ajay.com --yes

15. /usr/local/bin/kops update cluster --name useast1.dev.ajay.com --yes
kops validate cluster

16. kubectl get nodes

________________________________________________________________________________________________________________________________________________________________-
KUBERNETES COMMANDS:
-------------------
1. HOW TO CHECK NODES?
   $ kubectl get nodes

2. HOW TO CREATE DEPLOYMENT?
   $ kubectl create deployment dep1 --image=img_name

3. HOW TO CHECK DEPLOYMENT?
   $ kubectl get dployments

4. HOW TO CHECK PODS? (pods are not visible in the WorkerNodes, since they are logical Vmachines. Give the command in Master)
   $ Kubectl get pods

5. HOW TO GET MORE INFORMATION ABOUT PODS?
   $ kubectl get pods -o wide

6. HOW TO SCALE DEPLOYMENTS?
   $ kubectl scale deployment dep1 --replicas=3


10. HOW TO DELETE DEPLOYMENTS?
   $ kubectl delete deployment deployment_name

11. HOW TO DELETE NODE IN THE CLUSTER?
   $ kubectl delete node node_name


---> We have three different services 
a. clusterIP: If we need to create internal communication like communication between application and database we can use ClusterIP.

b. Nodeport: If we want to create communication between Application and customer we need to open Nodeport 
             $ kubectl expose deployment dep1 --type=NodePort -port=80 --target-port=80

c. Loadbalancer: 

7. HOW TO CREATE SERVICE?
   $ kubectl expose deployment dep1 --type=network --port=80 --target-port=80

8. HOW TO CHECK SERVICES?
   $ kubectl get services

9. HOW TO DELETE THE SERVICE?
   $ kubectl delete service service_name
=====================================================================================================================================================================
---> Writing YAML file to create Deployment:
vi deployment.yaml
 
apiversion: apps/v1beta1
kind: Deployment
metadata:
   name: my-apache-deployment
spec: 
 replicas: 1
 template: 
  metadata:
   labels:
      app: my-apache
   spec:
     containers: 
     - name: my-apache-container1
       image: httpd
       ports: 
     - containers: 80

$ kubectl apply -f deployment.yml 

---> Writing YAML file to create service:

apiversion: v1  
kind: service
metadata: 
   name: my-apache-service
spec:
   selector:
     app: my-apache
   type: LoadBalancer
   ports: 
      - name: my-apache-service
         port: 8080
         target-port: 80

---> INGRESS: An API object that manages external access to the services in a cluster, typically HTTP.
              Ingress exposes HTTP and HTTPS routes from outside the cluster to services within the cluster. Traffic routing is controlled by rules defined on the Ingress resource.
              Ingress may provide load balancing, SSL termination and name-based virtual hosting.
              An Ingress may be configured to give Services externally-reachable URLs, load balance traffic, terminate SSL / TLS, and offer name-based virtual hosting. 
              An Ingress controller is responsible for fulfilling the Ingress, usually with a load balancer, though it may also configure your edge router or additional frontends to help handle the traffic.
              An Ingress does not expose arbitrary ports or protocols. Exposing services other than HTTP and HTTPS to the internet typically uses a service of type Service.Type=NodePort or Service.Type=LoadBalancer


---> SSL Termination 
SSL termination is the process of decrypting traffic encrypted with SSL.

---> SSL:
Secure Sockets Layer, SSL, is a protocol for establishing encrypted and authenticated links between networked computers in order to keep internet connections secure and to safeguard sensitive data.
SSL identifies information about the website, such as the domain name and, optionally, information about the site’s owner. SSL may be establishing links between server-to-server or server-to-client.

---> SSL Termination:
SSL termination reduces the load on your servers while speeding up and simplifying data exchanges. SSL termination allows your application to handle more connections at a time.

SSL termination works by communicating with the encrypted traffic before the data reaches your server. Next, SSL termination decrypts and analyzes the traffic to ensure the data is not malicious. After, SSL termination encrypts the traffic again and the traffic is sent to your servers.

After you add your SSL certificate and private key, SSL termination handles the SSL decryption at the load balancer. Your load balancer can also act as a gateway between HTTP/2 client traffic and HTTP/1.0 or HTTP/1.1 backend applications this way. For more information about SSL termination, see How to Configure SSL Termination.


STORAGE FOR KUBERNETES CLUSTER:
==============================
Here we are going to attach shared file storage to the nodes in the cluster.
In the aws cloud services we have different storage services like EBS, EFS, S3.
We can attach these volumes to the nodes to store data.
we can't attach the EBS volume directly to nodes. But, with the help of NFS you can do it.
 
---> Network File System (NFS) is a distributed file system protocol that allows users to access files over a network like they access local storage.
1. First of all launch one ec2-instance (t2.micro). change the hostname to NFS and Install nfs in your server

Command: $ yum install nfs-utils
now, 
2. create EBS volume in aws console and attach it to nfs server.
3. In Terminal give lsblk command to check number volumes attached to your server.
4. Format the volume by giving the command 
 $ mkfs.ext4 /dev/xvdf
5. Create a directory with desired name (e.g: app)
 $ mkdir app
6. Mount the volume to that directory
 $ mount /dev/xvdf /app
7. vi /etc/exports

 /app ipaddress(rw,sync)
 /app ipaddress(rw,sync)
~
~
~
~
~
~
~
~
:wq!

8. Start nfs application
 $ systemctl start nfs

9. $ exportfs -a
10. $ exportfs
11. Open port number 2049 in all worker nodes and nfs server








---> Concept
There are two important things when it comes to assigning Pods to Nodes - "Affinity" and "AntiAffinity".

Affinity will basically select based on given criteria while anti-affinity will avoid based on given criteria.
With Affinity and Anti-affinity, you can use operators like In, NotIn, Exist, DoesNotExist, Gt and Lt. When you use NotIn and DoesNotExist, then it becomes anti-affinity.
Now, in Affinity/Antiaffinity, you have 2 choices - Node affinity/antiaffinity and Inter-pod affinity/antiaffinity

Node affinity/antiaffinity
Node affinity is conceptually similar to nodeSelector -- it allows you to constrain which nodes your pod is eligible to be scheduled on, based on labels on the node.


Inter-pod affinity/antiaffinity
Inter-pod affinity and anti-affinity allow you to constrain which nodes your pod is eligible to be scheduled based on labels on pods that are already running on the node rather than based on labels on nodes.

---> Your example:
Basically what you need is "Antiaffinity" and in that "Pod antiaffinity" instead of Node. So, your solution should look something like below (please note that since I do not have 3 Node cluster so I couldn't test this, so thin chances that you might have to do minor code adjustment):
 
 affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        labelSelector:
          - matchExpressions:
            - key: worker
              operator: In
              values:
              - log-scraper


======================================================================================================================================================================
                                                                           TROUBLESHOOTING KUBERENETES
======================================================================================================================================================================
---> The connection to the server 172.31.32.59:6443 was refused - did you specify the right host or port?
1. sudo -i
2. swapoff -a
3. exit
4. strace -eopenat kubectl version

---> kubeadm init --pod-network-cidr=10.244.0.0/16
[init] Using Kubernetes version: v1.26.3
[preflight] Running pre-flight checks
        [WARNING Hostname]: hostname "k8smaster" could not be reached
        [WARNING Hostname]: hostname "k8smaster": lookup k8smaster on 172.31.0.2:53: no such host
error execution phase preflight: [preflight] Some fatal errors occurred:
        [ERROR Port-10250]: Port 10250 is in use
[preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...`
To see the stack trace of this error execute with --v=5 or higher

* Debugging:un kubeadm reset first to undo all of the changes from the first time you ran it.

Then run systemctl restart kubelet

Finally, when you run kubeadm init you should no longer get the error.


If You Face Any Issues while Installing CRI on Ubuntu22.04
--->How to Install CRI-O on Ubuntu 22.04 / Ubuntu 20.04
Setup CRI-O Repository
Install the below packages to let apt have the support of the HTTPS method.
$ sudo apt update
$ sudo apt install -y apt-transport-https ca-certificates curl gnupg2 software-properties-common

NOTE: When writing this post, CRI-O packages are yet to be published for Ubuntu 22.04. However, the CRI-O packages for Ubuntu 20.04 are compatible with Ubuntu 22.04.

$ export OS_VERSION=xUbuntu_20.04
$ export CRIO_VERSION=1.23

Add the CRI-O’s GPG key to your system.
$ curl -fsSL https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/$OS_VERSION/Release.key | sudo gpg --dearmor -o /usr/share/keyrings/libcontainers-archive-keyring.gpg
$ curl -fsSL https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable:/cri-o:/$CRIO_VERSION/$OS_VERSION/Release.key | sudo gpg --dearmor -o /usr/share/keyrings/libcontainers-crio-archive-keyring.gpg

$ echo "deb [signed-by=/usr/share/keyrings/libcontainers-archive-keyring.gpg] https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/$OS_VERSION/ /" | sudo tee /etc/apt/sources.list.d/devel:kubic:libcontainers:stable.list
$ echo "deb [signed-by=/usr/share/keyrings/libcontainers-crio-archive-keyring.gpg] https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable:/cri-o:/$CRIO_VERSION/$OS_VERSION/ /" | sudo tee /etc/apt/sources.list.d/devel:kubic:libcontainers:stable:cri-o:$CRIO_VERSION.list

---> Install CRI-O on Ubuntu 22.04
First, update the repository index.
$ sudo apt update

Then, install CRI-O and run-time client using the apt command.
$ sudo apt install -y cri-o cri-o-runc

Start and enable CRI-O Service
Use the below commands to start the CR-O service.

$ sudo systemctl daemon-reload
$ sudo systemctl enable crio
$ sudo systemctl start crio

By now, the CRI-O service should be up and running.
$ sudo systemctl status crio

Output:

● crio.service - Container Runtime Interface for OCI (CRI-O)
     Loaded: loaded (/lib/systemd/system/crio.service; enabled; vendor preset: enabled)
     Active: active (running) since Fri 2022-04-29 12:04:02 EDT; 4min 31s ago
       Docs: https://github.com/cri-o/cri-o
   Main PID: 931 (crio)
      Tasks: 11
     Memory: 61.3M
        CPU: 1.189s
     CGroup: /system.slice/crio.service
             └─931 /usr/bin/crio

Apr 29 12:04:02 ubuntu-2204 crio[931]: time="2022-04-29 12:04:02.141059476-04:00" level=warning msg="The binary con>
Apr 29 12:04:02 ubuntu-2204 systemd[1]: Started Container Runtime Interface for OCI (CRI-O).
Apr 29 12:04:02 ubuntu-2204 crio[931]: time="2022-04-29 12:04:02.235133356-04:00" level=info msg="Successful


Install CNI Plugins For CRI-O
For the Pod to run, you need to set up CNI (Container Network Interface) and install CNI plugins. 
CNI configurations are stored in the /etc/cni/net.d directory and the bridge and loopback configurations are enabled by default which is enough for running Pods using CRI-O.

The default bridge configuration supports both IPv4 and IPv6. However, if you are setting up CNI on a node with IPv6 disabled, 
then I recommend you use 11-crio-ipv4-bridge.conf and remove the existing bridge configuration file 100-crio-bridge.conf.

In addition to the CNI configuration, you also need to install CNI plugins /opt/cni/bin for CRI-O. So, use the apt command to install CNI plugins.

$ sudo apt install -y containernetworking-plugins

Restart the CRI-O service.

$ sudo systemctl restart crio

Verify CRI-O Installation
Install CRI Tools to manage pods and containers.

sudo apt install -y cri-toolsCOPY
Now, check the CRIO-O version using the below command.

sudo crictl --runtime-endpoint unix:///var/run/crio/crio.sock versionCOPY
Output:
Version: 0.1.0
RuntimeName: cri-o
RuntimeVersion: 1.23.2
RuntimeApiVersion: v1alpha2

---> If k8s cluster is in not ready state means Give below command
$ systemctl restart kubelet


---> Other Example
$ kubeadm reset --cri-socket /var/run/crio/crio.sock
$ kubeadm init --cri-socket /var/run/crio/crio.sock {Runs only on Control plane(master_node)}
$ systemctl restart kubelet
$ kubeadm join 172.31.43.225:6443 --token uvpfb3.nwv9eu57m63omzmn         --discovery-token-ca-cert-hash sha256:78237f03d0dfe8bcd0b7e7ee0384dea99c81bfa21be705cbde230eeb7eb894b1 --cri-socket /var/run/crio/crio.sock

=====================================================================================================================================================================
                                                                       ANSIBLE
=====================================================================================================================================================================

---> Ansible is an Open-Source Tool. It is a Configuration Mnagement Tool and it Automates the Application Deployment.
---> Using YAML (Yet Another Markup Language) scripting language we will write Ansible Playbooks. Here by writing Playbooks we can perform the activites.
---> Ansible was developed by michael dehhan and ansible project started on feb 09 2012
---> Ansible was taken over by RedHat.
---> Ansible is Push Mechanism - Means if you perform anytask and any updates in application it push the notification.
---> Ansible Workflow:
     1.Firstly, you need to store the PrivateIp of nodes in Ansible master.
     2.You need to specify PrivateIp of Nodes in ansible Playbooks to make configuration in that nodes.
     3.How can we connect to Nodes? By using ssh application we can coonect to Node Servers  
       e.g: Password authentication, .ppk key, Publickey and privatekey.

---> ANSIBLE INSTALLATION:
    Pre-requisites: Git and python

$ sudo -i
$ sudo amazon-linux-extras install ansible2 -y
$ yum install git python python-pip python-level openssl -y

---> After installation of ansuble you can find ansible directory in etc directory there only you can be able to add hosts(Agent info)
$ vi /etc/ansible/hosts

[dev]
publicIP

[test]
publicIP

[amazon]
publIP1
publicIP2

:wq!

$ ansible -m ping all

Connection will be failed due, we need to create password less authentication between ansible master and slaves. 

$ ssh-keygen
$ cat /root/.ssh/id_rsa.pub

Copy the publickey and paste it in slaves Authorized_keys

Login to your slavesterminal

$ cd /root/.ssh
$ ls
$ vi authorized_keys

paste the publickey here and save it.

In Ansible Master:
$ ansible -m file -a "name=/tmp/ajay state=touch" all


$ vi /etc/ansible/ansible.cfg



Amazon Web Services Global Level Infrastructure----
                                REGIONS
                           AVAILABILITY ZONES
                              DATA CENTERS 
CATEGORISED INTO 4
1.North America
2.South America
3.Middle East {Africa, Europe}
4.Asia Pacific{Asia, Australia}
Suppose let us take ap-south-1, How they named this is in asia pacific Region and being mumbai is south most region 
5. Distance between the Data Centers is 100-150km. Bcoz suppose power supply issues, Security Issues, Cooling System, Natural Disasters(Prakruthi Vaiparityam). The data is stored in particular data center may lost.


What are smoke tests in it?
Smoke tests are a minimum set of tests run on each build. Smoke testing is a process where the software build is deployed to a quality assurance environment and is verified to ensure the stability of the application. Smoke Testing is also known as Confidence Testing or Build Verification Testing.
 
566083311859 --Account ID

